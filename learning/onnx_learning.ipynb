{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06442e54-c745-4379-9f41-3261c37f6cf7",
   "metadata": {},
   "source": [
    "# onnx\n",
    "> * 可执行序列，拓展性强，速度更快，通用性更强（可用python或c++，可在不同平台），占用存储更低\n",
    "> * 把模型变成onnx，如果有一个现有类就用optimum，否则用torch，\n",
    "> * 使用onnx进行推理，如果有现有类，用from_pretrained，否则用InferenceSession\n",
    "> * onnx使用gpu或者使用模型要注意版本问题，\n",
    "> * ONNX 的导出，使用的正是 TorchScript 的 trace 工具，所以能用onnx，不要用torchscript，不能用onnx部署时再用trochscript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d03eb-c38f-4e63-ae39-859d7adb07a3",
   "metadata": {},
   "source": [
    "## 一.把模型变成onnx结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470592e-38ac-4101-abbd-4e4093fdb6b4",
   "metadata": {},
   "source": [
    "### 1. 使用torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3ff35-1b63-4fba-abc5-30091a1323eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dummy_model_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")\n",
    "\n",
    "# export\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    tuple(dummy_model_input.values()),\n",
    "    f=\"torch-model.onnx\",\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}},\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042dbf1-fe5e-4faf-b001-c9c9dc268495",
   "metadata": {},
   "source": [
    "### 2. 使用transformers.onnx\n",
    "> * `pip install transformers[onnx] torch`\n",
    "> * 这个包不更新了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8097757-eb2c-471c-a1ed-7660b9581e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load model and tokenizer\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "feature = \"sequence-classification\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=tokenizer,\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"trfs-model.onnx\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf31b1-e45d-4f3b-864e-6e558b202bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "session = InferenceSession(\"onnx/model.onnx\")\n",
    "# ONNX Runtime expects NumPy arrays as input\n",
    "inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n",
    "outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900b074-e524-4f34-8d37-3d304863a1de",
   "metadata": {},
   "source": [
    "### 3. 使用 optimum\n",
    "> * `pip install optimum[onnxruntime]`\n",
    "> * 使用from_transformers=True自动把这个模型转为onnx\n",
    "> * 相当于把一些已有的模型类提前写好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5e055-04d9-4826-8fff-facb69ef10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2\n",
    "optimum_qa = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04df340-13a9-4aa8-8d26-b9c908e7cab7",
   "metadata": {},
   "source": [
    "## 二. 推理使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b5371-2a3e-4bdf-847b-3f0b28ae777d",
   "metadata": {},
   "source": [
    "### 1. 使用optimum.onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdb0e2-ba1f-4ff3-bd32-6fc28e29dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
    "inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a34b54-1eed-4698-b5fe-2c0c8db17edd",
   "metadata": {},
   "source": [
    "### 2. 使用onnxruntime.InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413b5c0-e2f2-4728-b552-b0b696016e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "session = InferenceSession(\"onnx/model.onnx\")\n",
    "# ONNX Runtime expects NumPy arrays as input\n",
    "inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n",
    "outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
