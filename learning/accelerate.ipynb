{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00148613-f591-4c8c-8013-7c7339714bfa",
   "metadata": {},
   "source": [
    "# accelerate\n",
    "> * 加速模型的训练和推理，优化大模型的训练和推理(deepspeed，混合精度训练，分片数据并行)\n",
    "> * Built on torch_xla and torch.distributed,\n",
    "> * 使用时要创建Accelerator类，不需要把模型和数据放在设备上（.to(device) or .cuda()），但是需要告诉要用哪几台机器来加速\n",
    "> * 能用trainer就用trainer，不能就用accelerate，再不能就用torch.ddp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abae207-875f-4448-bfc6-b7f4aac72e84",
   "metadata": {},
   "source": [
    "`accelerate config`： 配置使用accelerate的一些模型相关的参数，比如梯度累计之类的，会自动生成一个yaml文件\n",
    "\n",
    "`accelerate env`： 检测环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8a830-37d1-4dc4-8b8c-948e9cef00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "+ from accelerate import Accelerator\n",
    "+ accelerator = Accelerator()\n",
    "\n",
    "+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
    "+     model, optimizer, training_dataloader, scheduler\n",
    "+ )\n",
    "\n",
    "  for batch in training_dataloader:\n",
    "      optimizer.zero_grad()\n",
    "      inputs, targets = batch\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "      outputs = model(inputs)\n",
    "      loss = loss_function(outputs, targets)\n",
    "+     accelerator.backward(loss)\n",
    "      optimizer.step()\n",
    "      scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0d9fb-671d-4f02-ae37-8e253293ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch {my_script.py}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dcf38-88c0-4c7d-b524-691175fe694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_placement=False要完全停用自动设备放置，请在初始化 Accelerator时传递\n",
    "\n",
    "如果您手动将对象放置在适当的设备上，请在放置模型后小心创建优化器，accelerator.device否则您的训练将在 TPU 上失败。\n",
    "将所有与训练相关的对象（优化器、模型、训练数据加载器、学习率调度器）传递给 prepare()方法。这将确保一切都准备好进行培训。\n",
    "您可以完美地将数据加载器单独发送到prepare() ，但最好将模型和优化器一起发送到prepare()。\n",
    "loss.backward()用 替换该行accelerator.backward(loss)\n",
    "\n",
    "如果将验证数据加载器留在 prepare()方法之外，则可以在训练脚本中执行定期评估。在这种情况下，您需要 accelerator.device手动输入数据。\n",
    "\n",
    "要执行分布式评估，请将验证数据加载器发送到prepare() 方法：\n",
    "\n",
    "for inputs, targets in validation_dataloader:\n",
    "    predictions = model(inputs)\n",
    "    # Gather all predictions and targets\n",
    "    all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n",
    "    # Example of use with a *Datasets.Metric*\n",
    "    metric.add_batch(all_predictions, all_targets)\n",
    "  device_placement=False要完全停用自动设备放置，请在初始化 Accelerator时传递。  \n",
    "任何使用训练数据加载器长度的指令（例如，如果您需要总训练步数来创建学习率调度程序）都应该在调用prepare()之后进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812617e-6aed-4ec5-8286-214a5c3838b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "validation_dataloader = accelerator.prepare(validation_dataloader)\n",
    "\n",
    "for inputs, targets in validation_dataloader:\n",
    "    predictions = model(inputs)\n",
    "    # Gather all predictions and targets\n",
    "    all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n",
    "    # Example of use with a *Datasets.Metric*\n",
    "    metric.add_batch(all_predictions, all_targets)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad857661-eb73-4a01-ad3b-e3877c65a5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bb172-a62a-4a64-8208-6ce8d0278ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate config\n",
    "accelerate test\n",
    "accelerate test --config_file path_to_config.yaml\n",
    "accelerate launch path_to_script.py --args_for_the_script\n",
    "accelerate launch --config_file path_to_config.yaml path_to_script.py --args_for_the_script\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    # Is executed once per server\n",
    "    \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    # Is executed once only\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "accelerator.save(unwrapped_model.state_dict(), filename)\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=2)\n",
    "model, optimizer, training_dataloader = accelerator.prepare(model, optimizer, training_dataloader)\n",
    "\n",
    "for input, label in training_dataloader:\n",
    "    with accelerator.accumulate(model):\n",
    "        predictions = model(input)\n",
    "        loss = loss_function(predictions, label)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2118be1-e8e2-41c9-95fa-3c15809f9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "for batch in training_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a414350-dd74-47cf-be0e-2fca7c84611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "device = accelerator.device\n",
    "model.to(device)\n",
    "model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, training_dataloader, scheduler\n",
    ")\n",
    "\n",
    "for batch in training_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0a8cd-b59b-49bd-b637-a911e76b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "  from accelerate import Accelerator\n",
    "  \n",
    "+ def main():\n",
    "      accelerator = Accelerator()\n",
    "\n",
    "      model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
    "          model, optimizer, training_dataloader, scheduler\n",
    "      )\n",
    "\n",
    "      for batch in training_dataloader:\n",
    "          optimizer.zero_grad()\n",
    "          inputs, targets = batch\n",
    "          outputs = model(inputs)\n",
    "          loss = loss_function(outputs, targets)\n",
    "          accelerator.backward(loss)\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "\n",
    "+ if __name__ == \"__main__\":\n",
    "+     main()\n",
    "\n",
    "accelerate launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d255e68-77bc-49cb-93ef-2920d1e047d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...\n",
    "accelerate launch {script_name.py} --arg1 --arg2 ...\n",
    "\n",
    "accelerate launch --multi_gpu {script_name.py} {--arg1} {--arg2} ...\n",
    "accelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n",
    "accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...\n",
    "accelerate launch -h\n",
    "MIXED_PRECISION=\"fp16\" torchrun --nproc_per_node=2 --num_machines=1 {script_name.py} {--arg1} {--arg2} ...\n",
    "python -m accelerate.commands.launch --num_processes=2 {script_name.py} {--arg1} {--arg2}\n",
    "python -u -m accelerate.commands.launch --num_processes=2 {script_name.py} {--arg1} {--arg2}\n",
    "accelerate launch {script_name.py} {--arg1} {--arg2} ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991afd3-6717-426e-8a6f-d1300d43981c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f4554-2dd7-4fad-9ddd-39b07e1644c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac82b0-be82-4d98-9b69-64fb26450553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use Accelerate\n",
    "#\n",
    "# This example trains a Bert base model on GLUE MRPC\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - multi GPUS (using PyTorch distributed mode)\n",
    "#   - (multi) TPUs\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#\n",
    "# This example also demonstrates the checkpointing and sharding capabilities\n",
    "#\n",
    "# To run it in each of these various modes, follow the instructions\n",
    "# in the readme for examples:\n",
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\n",
    "MAX_GPU_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def training_function(config, args):\n",
    "    # Initialize accelerator\n",
    "    if args.with_tracking:\n",
    "        accelerator = Accelerator(\n",
    "            cpu=args.cpu, mixed_precision=args.mixed_precision, log_with=\"all\", project_dir=args.project_dir\n",
    "        )\n",
    "    else:\n",
    "        accelerator = Accelerator(cpu=args.cpu, mixed_precision=args.mixed_precision)\n",
    "\n",
    "    if hasattr(args.checkpointing_steps, \"isdigit\"):\n",
    "        if args.checkpointing_steps == \"epoch\":\n",
    "            checkpointing_steps = args.checkpointing_steps\n",
    "        elif args.checkpointing_steps.isdigit():\n",
    "            checkpointing_steps = int(args.checkpointing_steps)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Argument `checkpointing_steps` must be either a number or `epoch`. `{args.checkpointing_steps}` passed.\"\n",
    "            )\n",
    "    else:\n",
    "        checkpointing_steps = None\n",
    "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
    "    lr = config[\"lr\"]\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    seed = int(config[\"seed\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration\n",
    "    if args.with_tracking:\n",
    "        run = os.path.split(__file__)[-1].split(\".\")[0]\n",
    "        accelerator.init_trackers(run, config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # max_length=None => use the model max length (it's actually the default)\n",
    "        outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=None)\n",
    "        return outputs\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    "        )\n",
    "\n",
    "    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
    "    # transformers library\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # If the batch size is too big we use gradient accumulation\n",
    "    gradient_accumulation_steps = 1\n",
    "    if batch_size > MAX_GPU_BATCH_SIZE and accelerator.distributed_type != DistributedType.TPU:\n",
    "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
    "        batch_size = MAX_GPU_BATCH_SIZE\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
    "        max_length = 128 if accelerator.distributed_type == DistributedType.TPU else None\n",
    "        # When using mixed precision we want round multiples of 8/16\n",
    "        if accelerator.mixed_precision == \"fp8\":\n",
    "            pad_to_multiple_of = 16\n",
    "        elif accelerator.mixed_precision != \"no\":\n",
    "            pad_to_multiple_of = 8\n",
    "        else:\n",
    "            pad_to_multiple_of = None\n",
    "\n",
    "        return tokenizer.pad(\n",
    "            examples,\n",
    "            padding=\"longest\",\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    # Instantiate dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=EVAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "\n",
    "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
    "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
    "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
    "    model = model.to(accelerator.device)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "    # Instantiate scheduler\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs) // gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # We need to keep track of how many total steps we have iterated over\n",
    "    overall_step = 0\n",
    "    # We also need to keep track of the stating epoch so files are named properly\n",
    "    starting_epoch = 0\n",
    "\n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "            accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "            accelerator.load_state(args.resume_from_checkpoint)\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the most recent checkpoint\n",
    "            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "            dirs.sort(key=os.path.getctime)\n",
    "            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "        # Extract `epoch_{i}` or `step_{i}`\n",
    "        training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "        if \"epoch\" in training_difference:\n",
    "            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "            resume_step = None\n",
    "        else:\n",
    "            resume_step = int(training_difference.replace(\"step_\", \"\"))\n",
    "            starting_epoch = resume_step // len(train_dataloader)\n",
    "            resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "    # Now we train the model\n",
    "    for epoch in range(starting_epoch, num_epochs):\n",
    "        model.train()\n",
    "        if args.with_tracking:\n",
    "            total_loss = 0\n",
    "        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "            # We need to skip steps until we reach the resumed step\n",
    "            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "            overall_step += resume_step\n",
    "        else:\n",
    "            # After the first iteration though, we need to go back to the original dataloader\n",
    "            active_dataloader = train_dataloader\n",
    "        for step, batch in enumerate(active_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            batch.to(accelerator.device)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            # We keep track of the loss at each epoch\n",
    "            if args.with_tracking:\n",
    "                total_loss += loss.detach().float()\n",
    "            accelerator.backward(loss)\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            overall_step += 1\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                output_dir = f\"step_{overall_step}\"\n",
    "                if overall_step % checkpointing_steps == 0:\n",
    "                    if args.output_dir is not None:\n",
    "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    accelerator.save_state(output_dir)\n",
    "\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "            batch.to(accelerator.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n",
    "            metric.add_batch(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "            )\n",
    "\n",
    "        eval_metric = metric.compute()\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
    "        if args.with_tracking:\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"accuracy\": eval_metric[\"accuracy\"],\n",
    "                    \"f1\": eval_metric[\"f1\"],\n",
    "                    \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                step=epoch,\n",
    "            )\n",
    "\n",
    "        if checkpointing_steps == \"epoch\":\n",
    "            output_dir = f\"epoch_{epoch}\"\n",
    "            if args.output_dir is not None:\n",
    "                output_dir = os.path.join(args.output_dir, output_dir)\n",
    "            accelerator.save_state(output_dir)\n",
    "\n",
    "    if args.with_tracking:\n",
    "        accelerator.end_training()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp16\", \"bf16\", \"fp8\"],\n",
    "        help=\"Whether to use mixed precision. Choose\"\n",
    "        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
    "        \"and an Nvidia Ampere GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If the training should continue from a checkpoint folder.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_tracking\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to load in all available experiment trackers from the environment and use them for logging.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\".\",\n",
    "        help=\"Optional save directory where all checkpoint folders will be stored. Default is the current working directory.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--project_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=\"Location on where to store experiment tracking logs` and relevent project information\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n",
    "    training_function(config, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177ea42-4963-4eda-bbec-1c4a5a2c76f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde26daf-ffb4-4c8c-b54a-b330b40534cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights\n",
    "\n",
    "with init_empty_weights():\n",
    "    my_model = ModelClass(...)\n",
    "    \n",
    "\n",
    "\n",
    "with init_empty_weights():\n",
    "model = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e8073-e2f6-4392-be3b-3aeb488e6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"EleutherAI/gpt-j-6B\"\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
