{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% raw"
    }
   },
   "source": [
    "# tokenizerçš„å­¦ä¹ ï¼Œä»£ç æ¶‰åŠhuggingface-tokenizersåº“å’Œtansformersä¸­çš„tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€.åŸºç¡€\n",
    "1. ç®€ä»‹ï¼štokenizeræ˜¯åˆ†è¯å™¨ï¼Œåœ¨æˆ‘ä»¬è¦æŠŠå¥å­è¾“å…¥åˆ°æ¨¡å‹æ—¶ï¼Œæ¨¡å‹åªèƒ½æ¥æ”¶æ•°å­—çš„è¾“å…¥ï¼ˆæ¯”å¦‚input_idsä¸€äº›æ•°å­—åºåˆ—ï¼‰æ‰èƒ½è®¡ç®—ï¼Œæ‰€ä»¥tokenizerå°±æ˜¯æŠŠåŸå§‹ä¸€å¥äººç±»è¯­è¨€å˜æˆæ¨¡å‹èƒ½å¤Ÿæ¥æ”¶çš„è¾“å…¥ï¼Œè€Œå¹¿ä¹‰ä¸Šçš„tokenizeræ˜¯æŠŠå¥å­åˆ†æˆä¸€ä¸ªtokençš„åºåˆ—ï¼Œè¿™ä¹Ÿæ˜¯æ•´ä¸ªè¿‡ç¨‹ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼Œå…¶å®ä¹Ÿæ²¡é”™ï¼Œæ¯•ç«ŸæŠŠå¥å­åˆ†æˆtokenåï¼Œç„¶åå¯ä»¥é€šè¿‡é¢„å…ˆçš„vocab.txtçš„token2idæŠŠtokenæ˜ å°„æˆæ•°å­—\n",
    "2. tokenizersè¿™ä¸ªåº“æ˜¯ä½¿ç”¨rustå®ç°ï¼Œé€Ÿåº¦å¾ˆå¿«;ä½¿ç”¨tokenizersè¿™ä¸ªåº“é‡Œé¢çš„Tokenizerç±»çš„å¯ä»¥ä½¿ç”¨ailgn-trackï¼Œå¯¹é½è¿½è¸ªï¼Œæ‰¾åˆ°æ¯ä¸ªå¤„ç†åçš„tokenåœ¨åŸå§‹æ–‡æœ¬ä¸­çš„ä½ç½®ï¼Œ\n",
    "3. tokenizersè¿™ä¸ªåº“é‡Œé¢çš„ä¸»è¦ä»£ç æ˜¯ site-packages/tokenizers/tokenizers.cpython-310-x86_64-linux-gnu.soè¿™ä¸ªæ–‡ä»¶å®ç°çš„æ‰€æœ‰ç»†èŠ‚ï¼Œå…¶ä»–åªæœ‰pyiæ–‡ä»¶è¯´æ˜\n",
    "4. å¦‚æœåœ¨huggingfaceæ¨¡å‹ä¸­ç”¨åˆ°äº†tokenizersè¿™ä¸ªåº“ï¼Œé‚£ç›¸å¯¹åº”çš„tokenizerå°±æ˜¯fast-tokenizerï¼ˆä½¿ç”¨ruståŠ é€Ÿï¼Œå¯ä»¥è¿½è¸ªtokenä½ç½®ï¼‰ï¼Œæ‰€ä»¥å¦‚æœä¸€ä¸ªè½½å…¥çš„æ¨¡å‹ä½¿ç”¨çš„ä¸æ˜¯è¿™ä¸ªTokenizerç±»ï¼Œé‚£å®ƒå°±å¯èƒ½ä¸èƒ½è¿½è¸ªtokenä½ç½®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒ.åˆ›å»ºä¸€ä¸ªTokenizerç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![å›¾ç‰‡](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. normalization æ–‡æœ¬è§„èŒƒåŒ–ï¼šæŠŠä¸è§„èŒƒçš„æ–‡æœ¬å˜æˆè§„èŒƒçš„æ–‡æœ¬\n",
    "> * å¦‚æœé‡åˆ°çš„æ˜¯htmlï¼Œæœ€å¥½åœ¨htmlå‰å¤„ç†æˆå¥å­å½¢å¼ï¼Œè®°å½•ä¸‹offsetæ˜ å°„ï¼Œç„¶åå†è¿›è¡Œnormalization\n",
    "> * Normalizeræ˜¯åŸºç±»ï¼Œä¸»è¦ç”¨åˆ°normalize_stræ–¹æ³•è§„èŒƒåŒ–æ–‡æœ¬ï¼Œè¿˜æœ‰normalizeè¿½è¸ªï¼ˆä¸å¸¸ç”¨ï¼‰\n",
    "> * è§„èŒƒåŒ–çš„ä½¿ç”¨ï¼šç»„åˆå¤šä¸ªå¤„ç†\n",
    "> * unicodeç¼–ç ï¼šç”¨NFKD\n",
    ">> Unicode Normalizationï¼š https://xobo.org/unicode-normalization-nfd-nfc-nfkd-nfkc/\n",
    ">> \n",
    ">> unicodeä¸­å¯¹äºåŒä¸€æ–‡æœ¬çš„ä¸åŒè¡¨ç¤ºä¼šæœ‰ä¸åŒçš„unicodeï¼Œæ‰€ä»¥ä¼šå¯¹ç›¸åŒæ„ä¹‰çš„ä¸åŒå½¢å¼çš„unicodeè¿›è¡Œç­‰ä»·å¤„ç†ï¼Œæ¯”å¦‚â€˜å­—çš„Unicodeå€¼\\u2F4â€™ å’Œ â€˜æ–¹å­—Unicodeå€¼æ˜¯\\u65B9 â€™ï¼›ç­‰ä»·åˆ†ä¸ºï¼šæ ‡å‡†ç­‰ä»·å’Œå…¼å®¹ç­‰ä»·\n",
    ">> \n",
    ">> æ ‡å‡†ç­‰ä»·ï¼šä¿æŒè§†è§‰ä¸Šå’ŒåŠŸèƒ½ä¸Šç­‰ä»·ã€‚ä¾‹å¦‚ 'â‰ ' (\\u2260) æ ‡å‡†ç­‰ä»·äº '=' å’Œ 'Ì¸'(\\u003d\\u0338)çš„ç»„åˆã€‚åŒºåˆ«åœ¨äºä¸€ä¸ªæ˜¯ä¸€ä¸ªå­—ç¬¦ï¼Œä¸€ä¸ªæ˜¯ä¸¤ä¸ªå­—ç¬¦ã€‚\n",
    ">> \n",
    ">> å…¼å®¹ç­‰ä»·ï¼šå…¼å®¹ç­‰ä»·æ¯”æ ‡å‡†ç­‰ä»·èŒƒå›´æ›´å¹¿ï¼Œä¹Ÿæ›´å…³æ³¨çº¯æ–‡å­—çš„ç­‰ä»·ï¼Œå¹¶æŠŠä¸€äº›è¯­ä¹‰ä¸Šçš„ä¸åŒå½¢å¼å½’ç»“åœ¨ä¸€èµ·ã€‚å¦‚æœå­—ç¬¦æ˜¯æ ‡å‡†ç­‰ä»·ï¼Œé‚£ä¹ˆä»–ä»¬ä¸€å®šæ˜¯å…¼å®¹ç­‰ä»·ï¼Œåä¹‹å°±ä¸ä¸€å®šäº†ã€‚ä¾‹å¦‚ 'ãŠ‹ï¼‘ï½' (\\u328b\\uff11\\uff41) å¸¦åœ†åœˆçš„ç«ï¼Œå…¨è§’çš„æ•°å­—å’Œå­—æ¯å…¼å®¹ç­‰ä»·ä¸ 'ç«1a' (\\u706b\\u0031\\u0061)ã€‚\n",
    ">> \n",
    ">> å››ç§ç»„åˆï¼Œæ­¤åŸºç¡€ä¹‹åUnicodeåˆå®šä¹‰äº†ä¸¤ç§å½¢å¼ï¼Œå®Œå…¨åˆæˆï¼Œå®Œå…¨åˆ†è§£ã€‚ç”±æ­¤ç»„åˆå‡ºå››ç§å½¢å¼: NFCã€NFDã€NFKCã€NFKDã€‚\n",
    ">> \n",
    ">> C ä»£è¡¨ Composition å°±æ˜¯ç»„åˆçš„æ„æ€\n",
    ">> \n",
    ">> D ä»£è¡¨ Decomposition å°±æ˜¯åˆ†è§£çš„æ„æ€ã€‚\n",
    ">> \n",
    ">> K ä»£è¡¨ Compatibility å°±æ˜¯å…¼å®¹ç­‰ä»·æ¨¡å¼ï¼Œå¦‚æœä¸å¸¦Kå°±æ˜¯æ ‡å‡†ç­‰ä»·ã€‚\n",
    ">>\n",
    ">> åˆ†è§£å’Œç»„åˆä¸»è¦ç”¨äºæ˜¯å­—æ¯ç¬¦å·ã€‚æ±‰å­—æˆ‘æ²¡æ‰¾åˆ°å¯ä»¥åˆ†è§£å’Œç»„åˆçš„ä¾‹å­ã€‚æ±‰å­—çš„å…¼å®¹ç­‰ä»·åœºæ™¯å°±å¾ˆå¤šï¼Œé™¤äº†æˆ‘é‡åˆ°çš„éƒ¨é¦–â½…ï¼Œè¿˜æœ‰ ãŠ¥ ä¸ ä¸­ã€ ã¿ ä¸ æ ªå¼ä¼šç¤¾ã€å…¨è§’ä¸åŠè§’ã€‚ã€‚ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import BertNormalizer,Lowercase,NFC,NFD,NFKC,NFKD,Nmt,Normalizer,Precompiled,Replace,Sequence,Strip,StripAccents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /t \n",
      " HDdfÃ©ãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " HDdfeÌãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " HDdfÃ©ç«1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " HDdfeÌç«1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " hddfÃ©ãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "/t \n",
      " HDdfÃ©ãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "  /t \n",
      " HDdfÃ©ãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " HDdfeç«1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " HDdkÃ©ãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t \n",
      " HDdfÃ©ãŠ‹[NUM]äºŒäº”ä¸ƒäºŒ\n",
      "\n",
      "  /t   hddfeãŠ‹1234 äºŒ  äº”  ä¸ƒ  äºŒ  \n",
      "  /t \n",
      " HDdfeãŠ‹1234äºŒäº”ä¸ƒäºŒ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = '  /t \\n HDdfÃ©ãŠ‹1234äºŒäº”ä¸ƒäºŒ\\n'\n",
    "print(NFC().normalize_str(s))\n",
    "print(NFD().normalize_str(s))\n",
    "print(NFKC().normalize_str(s))\n",
    "print(NFKD().normalize_str(s))\n",
    "print(Lowercase().normalize_str(s))  # å°å†™\n",
    "print(Strip().normalize_str(s))  # strip\n",
    "\n",
    "print(StripAccents().normalize_str(s)) \n",
    "print(StripAccents().normalize_str(NFKD().normalize_str(s)))  # åˆ é™¤ unicode ä¸­çš„æ‰€æœ‰é‡éŸ³ç¬¦å·ï¼ˆä¸ NFD ä¸€èµ·ä½¿ç”¨ä»¥ä¿æŒä¸€è‡´æ€§ï¼‰\n",
    "\n",
    "print(Replace(\"f\", \"k\").normalize_str(s)) # è§„åˆ™æ›¿ä»£\n",
    "from tokenizers import Regex\n",
    "print(Replace(Regex(\"\\d+\"), '[NUM]').normalize_str(s)) # è§„åˆ™å¯ä»¥ç”¨regexï¼Œè¦æ›¿ä»£çš„åªèƒ½æ˜¯å­—ç¬¦ä¸²\n",
    "\n",
    "print(BertNormalizer(clean_text=True,handle_chinese_chars=True,strip_accents=True,lowercase=True).normalize_str(s))\n",
    "# bertå¤„ç†ï¼Œclean_textï¼šç§»é™¤ control characterså­—ç¬¦å¹¶ä¸”ç”¨ç»å…¸å­—ç¬¦æ›¿ä»£å„ç§ç©ºæ ¼(æ¯”å¦‚\\n),å¤„ç†ä¸­æ–‡ï¼Œåœ¨å„ä¸ªå­—ä¸­é—´æ·»åŠ ç©ºæ ¼ï¼Œå»é™¤é‡éŸ³ï¼Œ\n",
    "\n",
    "print(Sequence([NFD(), StripAccents()]).normalize_str(s)) # ç»„åˆå¤šä¸ªå¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. pre-tokenization é¢„æ ‡è®°åŒ–ï¼šæŠŠä¸€æ•´å¥è¯å…ˆæŒ‰ç®€å•è§„åˆ™åˆ‡åˆ†æˆtokenåºåˆ—ï¼Œä¿è¯æ¨¡å‹è¾“å…¥tokençš„æœ€å¤§å…ƒç´ å­˜åœ¨\n",
    "> * é¢„æ ‡è®°åŒ–æ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ›´å°çš„å¯¹è±¡çš„è¡Œä¸ºï¼Œè¿™äº›å¯¹è±¡ç»™å‡ºäº†è®­ç»ƒç»“æŸæ—¶æ ‡è®°çš„ä¸Šé™ã€‚ä¸€èˆ¬å®ä½“è¯†åˆ«éœ€è¦è¿™ä¸ªï¼Œ\n",
    "> * å’Œmodel-tokenizationä¸åŒçš„æ˜¯ï¼Œpre-tokenizationå°±æ˜¯ç”¨ç®€å•çš„è§„åˆ™å¯¹æ–‡æœ¬è¿›è¡Œæ‹†åˆ†è¾¾åˆ°æ¯ä¸ªtokenéƒ½æœ‰ä¸€ä¸ªå®Œæ•´labelçš„çŠ¶æ€ï¼ˆå®ä½“è¯†åˆ«ï¼‰æ¥ç¡®ä¿æ¨¡å‹ä¸ä¼šæœ‰å¤šä¸ªtokenç»„åˆå†ä¸€èµ·çš„tokenï¼Œæ¯”å¦‚ä½ ä¸æƒ³tokenä¸­åŒ…å«ä¸€ä¸ªç©ºæ ¼ï¼Œè¿™ä¸ªæ—¶å€™å°±å¯ä»¥ç”¨pretokenizerï¼Œè€Œmodel-tokenizationéœ€è¦å°†æ¯ä¸ªtokenæ‹†åˆ†è¾¾åˆ°èƒ½å¤Ÿå¾—åˆ°idçš„çŠ¶æ€\n",
    "> * ä½¿ç”¨pre_tokenize_stræ–¹æ³•,å®ƒèƒ½å¤Ÿå¾—åˆ°æ‹†åˆ†åæ¯ä¸ªtokençš„ä½ç½®offset\n",
    "> * ä½¿ç”¨é¢„åˆ†è¯å™¨æŠŠå•è¯æ‹†åˆ†ï¼Œæ¥ç¡®ä¿æœ€ç»ˆæ¨¡å‹å¾—åˆ°çš„tokenä¸èƒ½å¤§äºé¢„åˆ†è¯å™¨çš„tokenã€‚æ¯”å¦‚â€™it isâ€˜ ç»å¸¸å‡ºç°åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬æœ€åå¾—åˆ°çš„ç»“æœä¸éœ€è¦è¿™æ ·çš„ï¼Œæ‰€ä»¥å¯ä»¥ç”¨ç©ºæ ¼åˆ†è¯é¢„å…ˆæ‹†åˆ†è¿™ä¸ªï¼Œå¦‚æœæ²¡æœ‰å°†æˆ‘ä»¬çš„è¾“å…¥æ‹†åˆ†ä¸ºå•è¯çš„é¢„æ ‡è®°å™¨ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°ä¸å¤šä¸ªå•è¯é‡å çš„æ ‡è®°ï¼Œæ¯”å¦‚â€˜it isâ€™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Digits,ByteLevel,Whitespace,WhitespaceSplit,Punctuation,Metaspace,CharDelimiterSplit,Split,Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ä Hello', (0, 5)), ('Ä my', (5, 8)), ('Ä friend', (8, 15)), ('1', (15, 16)), (',', (16, 17)), ('Ä how', (17, 21)), ('Ä are', (21, 25)), ('Ä you', (25, 29)), ('23', (29, 31)), ('?', (31, 32))]\n",
      "[('Hello', (0, 5)), ('my', (6, 8)), ('friend1', (9, 16)), (',', (16, 17)), ('how', (18, 21)), ('are', (22, 25)), ('you23', (26, 31)), ('?', (31, 32))]\n",
      "[('Hello', (0, 5)), ('my', (6, 8)), ('friend1,', (9, 17)), ('how', (18, 21)), ('are', (22, 25)), ('you23?', (26, 32))]\n",
      "[('Hello my friend1', (0, 16)), (',', (16, 17)), (' how are you23', (17, 31)), ('?', (31, 32))]\n",
      "[('â–Hello', (0, 5)), ('â–my', (5, 8)), ('â–friend1,', (8, 17)), ('â–how', (17, 21)), ('â–are', (21, 25)), ('â–you23?', (25, 32))]\n",
      "[('Hello my f', (0, 10)), ('iend1, how a', (11, 23)), ('e you23?', (24, 32))]\n",
      "[('Hello my friend', (0, 15)), ('1', (15, 16)), (', how are you', (16, 29)), ('23', (29, 31)), ('?', (31, 32))]\n",
      "[('Hello my friend1, how ', (0, 22)), ('are', (22, 25)), (' you23?', (25, 32))]\n",
      "[('Hello my friend1, how are', (0, 25)), (' you23?', (25, 32))]\n",
      "[('Hello my friend1, how ', (0, 22)), ('are you23?', (22, 32))]\n",
      "[('Hello my friend1, how ', (0, 22)), (' you23?', (25, 32))]\n",
      "[('Hello my friend1, how ', (0, 22)), ('are', (22, 25)), (' you23?', (25, 32))]\n",
      "[('Hello my friend', (0, 15)), ('1', (15, 16)), (', how are you', (16, 29)), ('23', (29, 31)), ('?', (31, 32))]\n",
      "[('Hello', (0, 5)), ('my', (6, 8)), ('friend1', (9, 16)), (',', (16, 17)), ('how', (18, 21)), ('are', (22, 25)), ('you23', (26, 31)), ('?', (31, 32))]\n"
     ]
    }
   ],
   "source": [
    "s = 'Hello my friend1, how are you23?'\n",
    "print(ByteLevel().pre_tokenize_str(s))  # ä»¥ç©ºæ ¼æ‹†åˆ†ï¼Œå¹¶æŠŠæ‰€æœ‰bytesæ˜ å°„åˆ°å¯è§å­—ç¬¦ä¸Šï¼Œå¯¹äºéasciiå­—ç¬¦ä¼šä¸å¯è¯»ä½†æ˜¯æœ‰æ•ˆï¼Œ\n",
    "print(Whitespace().pre_tokenize_str(s)) # ä½¿ç”¨\\w+|[^\\w\\s]+ è¿›è¡Œæ‹†åˆ†\n",
    "print(WhitespaceSplit().pre_tokenize_str(s)) # ä½¿ç”¨ç©ºæ ¼æ‹†åˆ†\n",
    "\n",
    "print(Punctuation().pre_tokenize_str(s)) # ä½¿ç”¨æ ‡ç‚¹æ‹†åˆ†\n",
    "print(Metaspace().pre_tokenize_str(s)) # æ‹†åˆ†ç©ºæ ¼å¹¶ç”¨ç‰¹æ®Šå­—ç¬¦â€œ__â€æ›¿æ¢å®ƒä»¬ (U+2581)\n",
    "print(CharDelimiterSplit('r').pre_tokenize_str(s)) # ä»¥ç»™å®šå­—ç¬¦è¿›è¡Œæ‹†åˆ†\n",
    "print(Digits().pre_tokenize_str(s)) # ä»ä»»ä½•å…¶ä»–å­—ç¬¦ä¸­æ‹†åˆ†å‡ºæ•°å­—ã€‚\n",
    "\n",
    "# ä»¥è§„åˆ™è¿›è¡Œæ‹†åˆ†ï¼Œpatternæ˜¯å­—ç¬¦ä¸²æˆ–è§„åˆ™ï¼Œæ˜¯æŒ‡æ‹†åˆ†å‡ºæ¥çš„tokenï¼Œè¿™äº›tokenå¯ä»¥æœ‰å„ç§è¡Œä¸º\n",
    "# behavirorå¿…é¡»removedï¼Œisolatedï¼Œmerged_with_previousï¼Œmerged_with_nextã€‚contiguousä¸­çš„ä¸€ä¸ªï¼Œ\n",
    "# invert should be a boolean flag.\n",
    "print(Split(pattern = 'are', behavior = \"isolated\", invert = False).pre_tokenize_str(s)) # tokenç‹¬ç«‹\n",
    "print(Split(pattern = 'are', behavior = \"merged_with_previous\", invert = False).pre_tokenize_str(s)) # tokenå’Œå‰ä¸€ä¸ªç»„åˆ \n",
    "print(Split(pattern = 'are', behavior = \"merged_with_next\", invert = False).pre_tokenize_str(s)) # tokenå’Œåä¸€ä¸ªç»„åˆ\n",
    "print(Split(pattern = 'are', behavior = \"removed\", invert = False).pre_tokenize_str(s)) # tokenç§»é™¤\n",
    "print(Split(pattern = 'are', behavior = \"contiguous\", invert = False).pre_tokenize_str(s)) #tokenè¿ç»­\n",
    "from tokenizers import Regex\n",
    "print(Split(pattern = Regex(\"\\d+\"), behavior = \"contiguous\", invert = False).pre_tokenize_str(s)) #tokenè¿ç»­\n",
    "\n",
    "\n",
    "print(Sequence([Punctuation(), WhitespaceSplit()]).pre_tokenize_str(s)) # ç»„åˆpre-tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. model æ¨¡å‹åˆ†è¯ï¼šå¯¹ä¹‹å‰åˆ†å¥½çš„æ¯ä¸ªtokenåºåˆ—å†å°±è¡Œå•ä¸ªtokençš„åˆ‡åˆ†ï¼Œä¸€èˆ¬æ˜¯subwordç”¨åˆ°\n",
    "> * å®ƒæ˜¯å®é™…åˆ†è¯çš„æ ¸å¿ƒç®—æ³•ï¼Œæ˜¯é’ˆå¯¹æ¨¡å‹çš„ï¼Œæ‰€ä»¥å¦‚æœä½¿ç”¨Tokenizerï¼Œä¸€å®šè¦ä¼ å…¥è¿™ä¸ªå‚æ•°\n",
    "> * æ¨¡å‹çš„ä½œç”¨æ˜¯ä½¿ç”¨å®ƒå­¦åˆ°çš„è§„åˆ™å°†ä½ çš„â€œå•è¯â€æ‹†åˆ†æˆæ ‡è®°ã€‚å®ƒè¿˜è´Ÿè´£å°†è¿™äº›æ ‡è®°æ˜ å°„åˆ°æ¨¡å‹è¯æ±‡è¡¨ä¸­ç›¸åº”çš„ IDã€‚\n",
    "> * å®ƒå¯ä»¥é€šè¿‡è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œä¸åŒæ–¹æ³•éœ€è¦çš„å‚æ•°ä¸ä¸€æ ·\n",
    "> * åŸºæœ¬æ–¹æ³•ï¼šget_trainerï¼Œid_to_tokenï¼Œsaveï¼Œtoken_to_idï¼Œtokenize(è¾“å…¥tokençš„å­—ç¬¦ä¸²ï¼Œè¾“å‡ºA List of Token)ï¼Œfrom_file,read_file\n",
    "> * å®ƒçš„tokenizeæ–¹æ³•è¾“å…¥çš„æ˜¯ä¸€ä¸ªtokenï¼Œè€Œä¸æ˜¯ä¸€å¥è¯ï¼ŒæŠŠtokenè¿›è¡Œåˆ‡åˆ†å’Œidæ˜ å°„æ“ä½œ\n",
    "> * unk_tokenéœ€è¦è¢«vocabåŒ…å«\n",
    "> * å¦‚æœè¦ä½¿ç”¨ï¼Œç”¨BPEï¼ˆé¢‘ç‡ï¼‰æˆ–WordPieceï¼ˆè¯­è¨€æ¦‚ç‡ï¼‰ï¼Œæ‰€ä»¥å¦‚æœæœ‰è®­ç»ƒé›†ï¼Œç”¨wordpieceæ„Ÿè§‰å¯èƒ½æ›´å¥½ï¼Œä½†æ˜¯è¿™ä¸ªå¯èƒ½æ›´ä¾èµ–æ•°æ®é›†ï¼Œæ•°æ®é›†æ”¹å˜ï¼Œæ•ˆæœè¿…é€Ÿæ‹‰è·¨ï¼Ÿè€ŒBPEå¯èƒ½æ›´é€šç”¨ä¸€äº›ï¼Œä½†åœ¨ç‰¹æ®Šæ•°æ®é›†ä¸Šçš„æ•ˆæœå¯èƒ½ä¸å¦‚wordpiece\n",
    ">> * https://zhuanlan.zhihu.com/p/191648421\n",
    ">> * https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe\n",
    ">> * WordLevel: è¯åŸºæœ¬çš„æ–¹æ³•ï¼Œå°±æ˜¯æŠŠæ¯ä¸ªtokenæ˜ å°„åˆ°ä¸€ä¸ªid\n",
    ">> * BPE : subword-tokenization ç®—æ³•ï¼šä»å­—ç¬¦å¼€å§‹ï¼ŒåŒæ—¶å°†æœ€å¸¸è§çš„å­—ç¬¦ç»„åˆåœ¨ä¸€èµ·ï¼Œä»è€Œåˆ›å»ºæ–°çš„tokenï¼Œç„¶åè¿­ä»£åˆ›å»ºtokenï¼ŒBPE èƒ½å¤Ÿé€šè¿‡ä½¿ç”¨å¤šä¸ªå­è¯æ ‡è®°æ¥æ„å»ºå®ƒä»æœªè§è¿‡çš„è¯ï¼Œå› æ­¤éœ€è¦æ›´å°çš„è¯æ±‡è¡¨ï¼Œå…·æœ‰â€œunkâ€ï¼ˆæœªçŸ¥ï¼‰æ ‡è®°çš„æœºä¼šæ›´å°‘ã€‚,merges (List[Tuple[str, str]], optional) â€” A list of pairs of tokens,é¦–å…ˆå°†è¯åˆ†æˆå•ä¸ªå­—ç¬¦ï¼Œç„¶åä¾æ¬¡ç”¨å¦ä¸€ä¸ªå­—ç¬¦æ›¿æ¢é¢‘ç‡æœ€é«˜çš„ä¸€å¯¹å­—ç¬¦ ï¼Œç›´åˆ°å¾ªç¯æ¬¡æ•°ç»“æŸ,è¿™ä¸ªè¦ä¿è¯mergeé‡Œé¢çš„æ¯ä¸ªtokenéƒ½åœ¨vocabä¸­ä¸”åˆå¹¶ä¹‹åçš„tokenä¹Ÿè¦åœ¨vocabä¸­\n",
    ">> * Googleçš„Bertæ¨¡å‹åœ¨åˆ†è¯çš„æ—¶å€™ä½¿ç”¨çš„æ˜¯WordPieceç®—æ³•ã€‚ä¸BPEç®—æ³•ç±»ä¼¼ï¼ŒWordPieceç®—æ³•ä¹Ÿæ˜¯æ¯æ¬¡ä»è¯è¡¨ä¸­é€‰å‡ºä¸¤ä¸ªå­è¯åˆå¹¶æˆæ–°çš„å­è¯ã€‚ä¸BPEçš„æœ€å¤§åŒºåˆ«åœ¨äºï¼Œå¦‚ä½•é€‰æ‹©ä¸¤ä¸ªå­è¯è¿›è¡Œåˆå¹¶ï¼šBPEé€‰æ‹©é¢‘æ•°æœ€é«˜çš„ç›¸é‚»å­è¯åˆå¹¶ï¼Œè€ŒWordPieceé€‰æ‹©èƒ½å¤Ÿæå‡è¯­è¨€æ¨¡å‹æ¦‚ç‡æœ€å¤§çš„ç›¸é‚»å­è¯åŠ å…¥è¯è¡¨ã€‚ä½¿ç”¨è‘—åçš„## å‰ç¼€æ¥è¯†åˆ«ä½œä¸ºå•è¯ä¸€éƒ¨åˆ†çš„æ ‡è®°ï¼ˆå³ä¸æ˜¯å•è¯çš„å¼€å¤´ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import BPE,WordLevel,WordPiece,Unigram\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "token_list = 'hello my friend'.split()\n",
    "vocab  = ['he','llo','l','ll','o','hello','my','friend','[UNK]']\n",
    "vocab = {j:i for i,j in enumerate(vocab)}\n",
    "with open('./test_vocab.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(vocab, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 5, (0, 5))]\n",
      "[('[UNK]', 8, (0, 1)), ('[UNK]', 8, (1, 2)), ('llo', 1, (2, 5))]\n",
      "[('hello', 5, (0, 5))]\n"
     ]
    }
   ],
   "source": [
    "print([(i.value,i.id,i.offsets)for i in WordLevel(vocab=vocab,unk_token=\"[UNK]\").tokenize('hello')])\n",
    "print([(i.value,i.id,i.offsets)for i in BPE(vocab=vocab,merges=[('l','l'),('ll','o')],unk_token=\"[UNK]\").tokenize('hello')])\n",
    "\n",
    "print([(i.value,i.id,i.offsets)for i in WordPiece(vocab=vocab,unk_token=\"[UNK]\").tokenize('hello')])\n",
    "\n",
    "# uni_vocab = [(\"hello\", 0.2442),(\"my\", -0.2342),('[UNK]',-3)]\n",
    "# print([(i.value,i.id,i.offsets)for i in Unigram(vocab=uni_vocab).tokenize('hello')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. postprocessor åå¤„ç†ï¼šæŠŠæ¨¡å‹åˆ†è¯å¥½çš„tokenåºåˆ—å†åšä¸€äº›å¤„ç†ï¼Œæ¯”å¦‚åŠ ä¸Šä¸€äº›ç‰¹æ®Šçš„tokenï¼Œclsä¹‹ç±»çš„\n",
    "> * è¯·æ³¨æ„ï¼Œä¸é¢„åˆ†è¯å™¨æˆ–è§„èŒƒå™¨ç›¸åï¼Œæ‚¨æ— éœ€åœ¨æ›´æ”¹åå¤„ç†å™¨åé‡æ–°è®­ç»ƒåˆ†è¯å™¨ã€‚\n",
    "> * æ¨¡å‹è¾“å…¥æ¨¡æ¿åŒ–,ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦æ¥å¤„ç†å¥å­ï¼Œä¸€èˆ¬ä½¿ç”¨å¥å­æ¨¡æ¿,$Aè¡¨ç¤ºç¬¬ä¸€ä¸ªå¥å­,$Bç¬¬äºŒä¸ªå¥å­ã€‚,:1 æƒ³è¦è¾“å…¥çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œå¥å­çš„ç´¢å¼•ï¼šé»˜è®¤ä¸º 0 è¡¨ç¤ºæ‰€æœ‰å†…å®¹ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æ²¡æœ‰ $A:0ï¼‰ï¼Œ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing,BertProcessing,RobertaProcessing,ByteLevel\n",
    "post_processs = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. tokenizerï¼šæ ¹æ®ä¸Šé¢çš„ç»„ä»¶ç»„æˆä¸€ä¸ªTokenizer\n",
    "> * Tokenizerå¿…é¡»å«æœ‰model-mokenizerï¼ŒTokenizerå¯ä»¥éšæ—¶å˜æ›´normalizationï¼Œpre-tokenizer,post-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰.ä½¿ç”¨æ„å»ºå¥½çš„Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyl/disk/poetry_env/algorithms-ai-IgVB5U1f-py3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "s = \"Hello, ! How are you ğŸ˜ ?\"\n",
    "# è½½å…¥æ¨¡å‹çš„tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"/large_files/5T/huggingface_cache/pretrained_model/microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract/\"\n",
    "model_cache_dir = \"/large_files/5T/huggingface_cache/model\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint,\n",
    "                                               cache_dir=model_cache_dir)._tokenizer\n",
    "# è‡ªå®šä¹‰çš„tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. encode ç¼–ç ï¼šæŠŠåŸå§‹æ–‡æœ¬æ˜ å°„æˆæ¨¡å‹å¯ä»¥æ¥æ”¶çš„è¾“å…¥ï¼Œä¸»è¦æ˜¯ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, ! how are you ğŸ˜ ?\n",
      "[('hello', (0, 5)), (',', (5, 6)), ('!', (7, 8)), ('how', (9, 12)), ('are', (13, 16)), ('you', (17, 20)), ('ğŸ˜', (21, 22)), ('?', (23, 24))]\n",
      "[[<tokenizers.Token object at 0x7fc2a03a9700>, <tokenizers.Token object at 0x7fc2a03a88f0>], [<tokenizers.Token object at 0x7fc2a03a8670>], [<tokenizers.Token object at 0x7fc2a03a8da0>], [<tokenizers.Token object at 0x7fc2a03a8a80>], [<tokenizers.Token object at 0x7fc2a03a8850>], [<tokenizers.Token object at 0x7fc2a03a8e90>], [<tokenizers.Token object at 0x7fc2109ce3d0>], [<tokenizers.Token object at 0x7fc2109ce420>]]\n",
      "[<tokenizers.Token object at 0x7fc2a03a9700>, <tokenizers.Token object at 0x7fc2a03a88f0>, <tokenizers.Token object at 0x7fc2a03a8670>, <tokenizers.Token object at 0x7fc2a03a8da0>, <tokenizers.Token object at 0x7fc2a03a8a80>, <tokenizers.Token object at 0x7fc2a03a8850>, <tokenizers.Token object at 0x7fc2a03a8e90>, <tokenizers.Token object at 0x7fc2109ce3d0>, <tokenizers.Token object at 0x7fc2109ce420>]\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', 'attention_mask', 'char_to_token', 'char_to_word', 'ids', 'merge', 'n_sequences', 'offsets', 'overflowing', 'pad', 'sequence_ids', 'set_sequence_id', 'special_tokens_mask', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'truncate', 'type_ids', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Encoding\n",
    "n_s = bert_tokenizer.normalizer.normalize_str(s)\n",
    "pt_s = bert_tokenizer.pre_tokenizer.pre_tokenize_str(n_s)\n",
    "m_s = [bert_tokenizer.model.tokenize(i[0]) for i in pt_s]\n",
    "print(n_s)\n",
    "print(pt_s)\n",
    "print(m_s)\n",
    "a = Encoding()\n",
    "all_m_s = [j for i in m_s for j in i]\n",
    "print(all_m_s)\n",
    "# a.tokens = [i for i in all_m_s]\n",
    "# a.values = [i.value for i in all_m_s]\n",
    "print(dir(a))\n",
    "bert_tokenizer.post_processor.process(a)  # æœªçŸ¥ä»£ç ,è€Œä¸”å…¶ä¸­è¿˜æ¶‰åŠåˆ°padï¼Œtruncationç­‰æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "[Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.encode(s))\n",
    "print(bert_tokenizer.encode_batch([s]*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. decode ç¼–ç ï¼šæŠŠæ¨¡å‹çš„è¾“å…¥idsè§£ç æˆåŸå§‹æ–‡æœ¬\n",
    "> * é¦–å…ˆå°† ID è½¬æ¢å›æ ‡è®°ï¼ˆä½¿ç”¨æ ‡è®°å™¨çš„è¯æ±‡è¡¨ï¼‰å¹¶åˆ é™¤æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œç„¶åç”¨ç©ºæ ¼è¿æ¥è¿™äº›æ ‡è®°\n",
    "> * è§£ç å™¨å’Œç¼–ç å™¨è¦å¯¹åº”,è™½ç„¶å®ƒä¹Ÿå¯ä»¥éšä¾¿æ¢ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ¨¡å‹æ·»åŠ äº†ç‰¹æ®Šå­—ç¬¦æ¥è¡¨ç¤ºç»™å®šâ€œå•è¯â€çš„å­æ ‡è®°ï¼ˆå¦‚ WordPiece ä¸­çš„â€œ##â€ï¼‰ï¼Œæ‚¨å°†éœ€è¦è‡ªå®šä¹‰è§£ç å™¨ä»¥æ­£ç¡®å¤„ç†å®ƒä»¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.decoders.WordPiece object at 0x7fc2a220a9a0>\n",
      "hello,! how are you?\n",
      "['hello,! how are you?', 'hello,! how are you?']\n",
      "hel##lo,!howareyou?\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.decoders import BPEDecoder,ByteLevel,Metaspace,WordPiece,CTC\n",
    "print(bert_tokenizer.decoder)\n",
    "print(bert_tokenizer.decode(bert_tokenizer.encode(s).ids))\n",
    "print(bert_tokenizer.decode_batch([bert_tokenizer.encode(s).ids]*2))\n",
    "\n",
    "bert_tokenizer.decoder = BPEDecoder()\n",
    "print(bert_tokenizer.decode(bert_tokenizer.encode(s).ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å…¶ä»–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * add_special_tokens:å¦‚æœè¿™äº›æ ‡è®°å·²ç»æ˜¯è¯æ±‡è¡¨çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒåªä¼šè®© Tokenizer çŸ¥é“å®ƒä»¬ã€‚å¦‚æœå®ƒä»¬ä¸å­˜åœ¨ï¼Œåˆ™ Tokenizer ä¼šåˆ›å»ºå®ƒä»¬ï¼Œå¹¶ç»™å®ƒä»¬ä¸€ä¸ªæ–°çš„ idã€‚è¿™äº›ç‰¹æ®Šæ ‡è®°æ°¸è¿œä¸ä¼šè¢«æ¨¡å‹å¤„ç†ï¼ˆå³ä¸ä¼šè¢«æ‹†åˆ†æˆå¤šä¸ªæ ‡è®°ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è§£ç æ—¶å°†å®ƒä»¬ä»è¾“å‡ºä¸­ç§»é™¤ã€‚\n",
    "> * add_tokensä»…å½“è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨ç»™å®šæ ‡è®°æ—¶æ‰æ·»åŠ å®ƒä»¬ã€‚ç„¶åæ¯ä¸ªä»¤ç‰Œéƒ½ä¼šè·å¾—ä¸€ä¸ªæ–°çš„å±æ€§ IDã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.enable_padding(direction = 'right',pad_id = 0,pad_type_id = 0,pad_token = '[PAD]',length = None,pad_to_multiple_of = None)\n",
    "bert_tokenizer.enable_truncation(max_length=512,stride = 0,strategy = 'longest_first',direction = 'right' )\n",
    "bert_tokenizer.add_special_tokens([\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"])\n",
    "bert_tokenizer.add_tokens([\"<e2>\", \"</e4>\", \"<e3>\", \"</e2>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››.è®­ç»ƒå®šä¹‰å¥½çš„tokenizer\n",
    "> * è®­ç»ƒtokenizeræ˜¯ä¸€ä¸ªç»Ÿè®¡è¿‡ç¨‹ï¼Œå®ƒè¯•å›¾è¯†åˆ«ç»™å®šè¯­æ–™åº“ä¸­æœ€é€‚åˆé€‰æ‹©çš„å­è¯ï¼Œç”¨äºé€‰æ‹©å®ƒä»¬çš„ç¡®åˆ‡è§„åˆ™å–å†³äºæ ‡è®°åŒ–ç®—æ³•ã€‚å®ƒæ˜¯ç¡®å®šæ€§çš„ï¼Œè¿™æ„å‘³ç€åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ç›¸åŒçš„ç®—æ³•è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. é€‰æ‹©ä¸€ä¸ªtokenizerè®­ç»ƒå™¨\n",
    "> * å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªtokenizeræˆ–è€…è½½å…¥ä¸€ä¸ªtokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers,decoders,normalizers\n",
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "wordpiece_tokenizer.normalizer = normalizers.NFKC()\n",
    "wordpiece_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "wordpiece_tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# checkpoint = \"/large_files/5T/huggingface_cache/pretrained_model/microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract/\"\n",
    "# model_cache_dir = \"/large_files/5T/huggingface_cache/model\"\n",
    "# wordpiece_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint,\n",
    "#                                                cache_dir=model_cache_dir).backend_tokenizer\n",
    "\n",
    "wordpiece_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "token_list = 'Hello my friend'.split()\n",
    "vocab  = ['hello','my','friend','[UNK]']\n",
    "vocab  = {j:i for i,j in enumerate(vocab)}\n",
    "print(vocab)\n",
    "WordPiece(vocab=vocab,unk_token=\"[UNK]\").tokenize('hello my')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. é€‰æ‹©tokenizer-modelå¯¹åº”çš„trainer\n",
    "> * æ³¨æ„trainerå’Œtokenizerå¯¹åº”ã€‚æ¯ä¸ªtrainerå¯ä»¥è®¾ç½®ä¸€äº›ç‰¹å®šçš„å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordpiece_trainer =  WordPieceTrainer(vocab_size=90,min_frequency=3,\n",
    "                     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. é…ç½®è¦ä½¿ç”¨çš„é¢„æ–™\n",
    "> * train_from_iteratoréœ€è¦æ•°æ®æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–è€…è¿­ä»£çš„å­—ç¬¦ä¸²\n",
    "> * trainéœ€è¦çš„æ˜¯ä¸€äº›æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"Beautiful is better than ugly.\",\n",
    "    \"Explicit is better than implicit.\",\n",
    "    \"Simple is better than complex.\",\n",
    "    \"Complex is better than complicated.\",\n",
    "    \"Flat is better than nested.\",\n",
    "    \"Sparse is better than dense.\",\n",
    "    \"Readability counts.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordpiece_tokenizer.train_from_iterator(data*20, trainer=wordpiece_trainer)\n",
    "wordpiece_tokenizer.train_from_iterator(iter(data), trainer=wordpiece_trainer)\n",
    "\n",
    "# wordpiece_tokenizer.train(files, trainer=wordpiece_trainer)\n",
    "\n",
    "wordpiece_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordpiece_tokenizer.save(\"test_bert_vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. è½½å…¥è®­ç»ƒå¥½çš„tokenizerå¹¶ä¸”ä½¿ç”¨\n",
    "> * saveå¯¹åº”from_file\n",
    "> * from_pretrainedä½¿ç”¨hubä¸Šçš„tokenizer.json\n",
    "> * ä½¿ç”¨_tokenizerè¿›è¡Œæ›¿æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17, 3]\n",
      "[32, 56, 38, 47, 49, 45, 43, 57, 49, 37, 68, 71, 69, 32, 49, 50, 37, 51, 5]\n"
     ]
    }
   ],
   "source": [
    "s = \"Beautiful is better than ugly.\"\n",
    "new_token = Tokenizer.from_file(\"test_bert_vocab.json\")\n",
    "\n",
    "checkpoint = \"/large_files/5T/huggingface_cache/pretrained_model/microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract/\"\n",
    "model_cache_dir = \"/large_files/5T/huggingface_cache/model\"\n",
    "bt_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint,\n",
    "                                               cache_dir=model_cache_dir)\n",
    "print(bt_tokenizer.encode(s))\n",
    "bt_tokenizer._tokenizer = new_token\n",
    "print(bt_tokenizer.encode(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## äº”.hugging-faceä¸­çš„tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.è®­ç»ƒ\n",
    "> * huggingfaceæœ‰ä¸€ä¸ªå¯ä»¥è®­ç»ƒä¸ç°æœ‰æ ‡è®°å™¨ç›¸åŒç‰¹å¾çš„æ–°æ ‡è®°å™¨çš„è½¬æ¢å™¨ï¼šAutoTokenizer.train_new_from_iteratorï¼ˆï¼‰\n",
    "> * ä½¿ç”¨from_pretrainedå’Œsave_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', '##aut', '##if', '##ul', 'is', 'better', 'than', 'ug', '##ly', '.']\n",
      "\n",
      "\n",
      "\n",
      "['beautiful', 'is', 'better', 'than', 'ugly', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer/tokenizer_config.json',\n",
       " 'code-search-net-tokenizer/special_tokens_map.json',\n",
       " 'code-search-net-tokenizer/vocab.txt',\n",
       " 'code-search-net-tokenizer/added_tokens.json',\n",
       " 'code-search-net-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"/large_files/5T/huggingface_cache/pretrained_model/microsoft--BiomedNLP-PubMedBERT-base-uncased-abstract/\"\n",
    "model_cache_dir = \"/large_files/5T/huggingface_cache/model\"\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint,\n",
    "                                               cache_dir=model_cache_dir)\n",
    "s = \"Beautiful is better than ugly.\"\n",
    "print(old_tokenizer.tokenize(s))\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(data, 52000)\n",
    "print(new_tokenizer.tokenize(s))\n",
    "\n",
    "#æ–°è®­ç»ƒçš„åˆ†è¯å™¨å¯ä»¥ä¿å­˜èµ·æ¥ï¼Œæ³¨æ„è¿™é‡Œç”¨çš„æ˜¯AutoTokenizer\n",
    "new_tokenizer.save_pretrained( \"code-search-net-tokenizer\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.ä½¿ç”¨\n",
    "> * tokenizerä¸»è¦æ ¹æ®ä¸€ç»„è§„åˆ™å°†æ–‡æœ¬æ‹†åˆ†ä¸ºtokenï¼Œç„¶ååœ¨å¥å­ä¸­æ·»åŠ äº›ç‰¹æ®Štokenï¼Œç„¶åå°†æ‰€æœ‰tokenæ˜ å°„ä¸ºæ•°å­—ï¼Œç„¶åæŠŠè¿™äº›æ•°å­—è½¬æ¢æˆå¼ é‡\n",
    "> * callçš„æµç¨‹ï¼šraw_text-ï¼ˆ_tokenizer.tokenizeæ–¹æ³•ï¼‰->tokens --> special tokensï¼ˆç”±prepare_for_modelæ–¹æ³•æ·»åŠ ï¼‰ -ï¼ˆconvert_tokens_to_idsï¼‰->input_ids -(prepare_for_model)-->model-(decode)->\n",
    "> * callçš„è¿”å›,å®é™…æ˜¯encondingå¯¹è±¡ï¼Œå¯ä»¥è¿”å›å¾ˆå¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬return_offsets_mappingï¼š\n",
    ">> * input_idè¡¨ç¤ºå¥å­ä¸­æ¯ä¸ªtokendçš„indicesæ˜ å°„ï¼ˆconvert_tokens_to_idsï¼‰\n",
    ">> * attention_maskæ˜¯å¦å¯¹åº”ä½ç½®çš„tokenåº”è¯¥è¢«åŠ å…¥æ³¨æ„åŠ›è®¡ç®—ï¼ˆ1è¡¨ç¤ºä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„æ—¶å€™è®¡ç®—å®ƒï¼‰,ä¸º0çš„ä¸è¦è®¡ç®—attention\n",
    ">> * token_type_ids: å°±æ˜¯å½“è¾“å…¥æ˜¯å¥½å‡ å¥è¯æ—¶è¡¨ç¤ºè¿™ä¸ªtokenå±äºå“ªå¥è¯,ä¸»è¦ç”¨äºå¥å­å¯¹\n",
    ">> * offset...\n",
    ">> * encodeçš„æ—¶å€™å¦‚æœç¢°åˆ°ä¸åœ¨vocabä¸­çš„ï¼Œå°±ä¼šæŠ¥é”™ï¼Œé™¤éé‡Œé¢æœ‰é¢„å¤„ç†\n",
    "> * padding:\n",
    ">> * padding(é»˜è®¤False)ï¼šç”±äºå„å¥è¯æ€»æ˜¯é•¿åº¦ä¸ä¸€è‡´ï¼Œè€Œæ¨¡å‹éœ€è¦è¾“å…¥çš„æ˜¯å¼ é‡è¦ä¿è¯ä¸€ä¸ªbatchçš„é•¿åº¦ä¸€æ ·ï¼Œæ‰€ä»¥paddingä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„padding-tokenï¼ˆä¸€èˆ¬0ï¼‰æ¥è¡¥é½çŸ­çš„å¥å­çš„tokenï¼Œç›´åˆ°å’Œä¸€ä¸ªbatchä¸­æœ€é•¿å¥å­çš„tokensé•¿åº¦ä¸€è‡´\n",
    ">> * ä½¿ç”¨paddingæ—¶ï¼Œè¡¥é½çš„tokençš„attention-maskå¯¹åº”çš„ä½ç½®ä¸º0ï¼Œ\n",
    ">> * paddingé»˜è®¤longest ï¼Œå¯ä»¥æŒ‡å®šâ€™max_lengthâ€˜\n",
    "> * truncation:\n",
    ">> * ï¼ˆé»˜è®¤Falseï¼‰ï¼šå¥å­å¤ªé•¿æ‰€ä»¥æˆªæ–­ï¼Œä½¿ç”¨Trueè¡¨ç¤ºæ¨¡å‹æœ€å¤§å¯æ¥å—çš„é•¿åº¦\n",
    ">> *  æŒ‰æ¨¡å‹max_lengthè¾“å…¥è¿›è¡Œæˆªæ–­,é»˜è®¤Falseï¼ŒæŒ‰model_max_lengthæˆ–è€…è¾“å…¥æœ€å¤§é•¿åº¦æˆ–max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  1765, 19744,  1733,  1703,  1744,  3387,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[ 0,  0],\n",
      "         [ 0,  2],\n",
      "         [ 2,  5],\n",
      "         [ 5,  7],\n",
      "         [ 7,  9],\n",
      "         [10, 12],\n",
      "         [13, 19],\n",
      "         [ 0,  0]]])}\n",
      "{'input_ids': [2, 1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_encodings', '_n_sequences', 'char_to_token', 'char_to_word', 'clear', 'convert_to_tensors', 'copy', 'data', 'encodings', 'fromkeys', 'get', 'is_fast', 'items', 'keys', 'n_sequences', 'pop', 'popitem', 'sequence_ids', 'setdefault', 'to', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'update', 'values', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n",
      "<bound method BatchEncoding.word_ids of {'input_ids': [2, 1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n",
      "{'input_ids': [[2, 1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17, 3], [2, 1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "##############################\n",
      "Ã— [CLS] [SEP] [MASK]! # $\n",
      "['Ã—', '[CLS]', '[SEP]', '[MASK]', '!', '#', '$']\n",
      "Ã— [CLS] [SEP] [MASK]! # $\n",
      "##############################\n",
      "[2, 1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17, 3]\n",
      "['be', '##aut', '##if', '##ul', 'is', 'better', 'than', 'ug', '##ly', '.']\n",
      "[1765, 19744, 1733, 1703, 1744, 3387, 1981, 16110, 1716, 17]\n",
      "##############################\n",
      "3\n",
      "##############################\n",
      "##############################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Beautiful is better than ugly.\"\n",
    "print(old_tokenizer(s, return_offsets_mapping=True,padding=True,  max_length=8, truncation=True, return_tensors='pt' ))  # ç­‰äºencode_plus,è¿”å›çš„æ˜¯encodeingå¯¹è±¡\n",
    "print(old_tokenizer.encode_plus(s))\n",
    "print(dir(old_tokenizer(s)))\n",
    "print(old_tokenizer(s).word_ids)\n",
    "print(old_tokenizer.batch_encode_plus([s]*2))\n",
    "\n",
    "print('#'*30)\n",
    "print(old_tokenizer.decode([102, 2, 3, 4, 5, 6, 7]))\n",
    "print(old_tokenizer.convert_ids_to_tokens([102, 2, 3, 4, 5, 6, 7]))\n",
    "print(old_tokenizer.convert_tokens_to_string(old_tokenizer.convert_ids_to_tokens([102, 2, 3, 4, 5, 6, 7])))\n",
    "\n",
    "print('#'*30)\n",
    "print(old_tokenizer.encode(s))\n",
    "print(old_tokenizer.tokenize(s))\n",
    "print(old_tokenizer.convert_tokens_to_ids(old_tokenizer.tokenize(s)))\n",
    "\n",
    "print('#'*30)\n",
    "print(old_tokenizer._tokenizer.token_to_id(\"[SEP]\"))\n",
    "\n",
    "\n",
    "print('#'*30)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[C1]','[C2]','[C3]','[C4]']}\n",
    "num_added_toks = old_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print('#'*30)\n",
    "new_tokens = ['token1', 'token2'] \n",
    "old_tokenizer.add_tokens(new_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…¶ä»–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "print(1)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# éœ€è¦è®¾ç½®is_split_into_words=Trueå°†å•è¯æ ‡è®°ä¸ºå­è¯\n",
    "# ä½¿ç”¨tokenizerçš„æ—¶å€™å¦‚æœä¼ å…¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé‚£ä¼šæŠŠåˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ éƒ½çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œå¯¹è¿™ä¸ªæ ·æœ¬è¿›è¡Œåˆ‡åˆ†\n",
    "# ä½†å¦‚æœä½¿ç”¨is_split_into_words=Trueï¼Œåˆ™ä¼šæŠŠæœ€å°å•å…ƒçš„åˆ—è¡¨çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œè€Œä¸”ä¼šå¯¹è¿™ä¸ªæ ·æœ¬ä¸­çš„æ¯ä¸ªtokenè¿›ä¸€æ­¥åˆ‡åˆ†\n",
    "# tokenizer('a')  # è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ™å°†è¿™ä¸ªå­—ç¬¦ä¸²çœ‹ä½œä¸€ä¸ªæ ·æœ¬\n",
    "# {'input_ids': [101, 1037, 102], 'attention_mask': [1, 1, 1]}\n",
    "# tokenizer('a','b')  # è¾“å…¥å¤šä¸ªå­—ç¬¦ä¸²ï¼Œå°†è¿™äº›å­—ç¬¦ä¸²çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªå­—ç¬¦ä¸²çœ‹ä½œä¸€ä¸ªtoken\n",
    "# {'input_ids': [101, 1037, 102, 1038, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n",
    "# tokenizer(['a'])  # è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼ŒæŠŠè¿™ä¸ªåˆ—è¡¨çœ‹ä½œå¤šä¸ªæ ·æœ¬ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œä¸èƒ½æ˜¯åˆ—è¡¨\n",
    "# {'input_ids': [[101, 1037, 102]], 'attention_mask': [[1, 1, 1]]}\n",
    "# tokenizer(['a','b']) # è¾“å…¥ä¸€ä¸ªå¤šä¸ªå­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼ŒæŠŠè¿™ä¸ªåˆ—è¡¨çœ‹ä½œå¤šä¸ªæ ·æœ¬ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œä¸èƒ½æ˜¯åˆ—è¡¨\n",
    "# {'input_ids': [[101, 1037, 102], [101, 1038, 102]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n",
    "# tokenizer([['a'],['b']]) # è¾“å…¥ä¸€ä¸ªå¤šä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼ŒæŠ¥é”™,é™¤éä½¿ç”¨is_split_into_words=Trueï¼ŒæŠŠé‡Œé¢çš„æ¯ä¸ªåˆ—è¡¨çœ‹ä½œä¸€ä¸ªæ ·æœ¬\n",
    "# tokenizer([['a'],['b']],is_split_into_words=True)\n",
    "# {'input_ids': [[101, 1037, 102], [101, 1038, 102]], 'attention_mask': [[1, 1, 1], [1, 1, 1]]}\n",
    "# tokenizer(['a'],['b']) # è¾“å…¥æ˜¯å¤šä¸ªåˆ—è¡¨ï¼ŒæŠŠæ¯ä¸ªåˆ—è¡¨çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œ\n",
    "# {'input_ids': [[101, 1037, 102, 1038, 102]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n",
    "# tokenizer(['a','c'],['b','d']) # è¾“å…¥æ˜¯å¤šä¸ªåˆ—è¡¨ï¼ŒæŠŠè¿™äº›åˆ—è¡¨çœ‹ä½œå¤šä¸ªæ ·æœ¬ï¼Œ\n",
    "# {'input_ids': [[101, 1037, 102, 1038, 102], [101, 1039, 102, 1040, 102]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n",
    "\n",
    "# æ‰€ä»¥è¦è¾“å…¥å•ä¸ªæ ·æœ¬\n",
    "# tokenizer('a')\n",
    "# tokenizer('a','b')\n",
    "\n",
    "# å¦‚æœè¦è¾“å…¥å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬\n",
    "# tokenizer(['a','b'])\n",
    "# tokenizer(['a','c'],['b','d'])\n",
    "\n",
    "# å¦‚æœå•ä¸ªæ ·æœ¬æ˜¯list[tokens]\n",
    "# å¦‚æœä½¿ç”¨tokenizer(example[\"tokens\"])æˆ–è€… tokenizer(*[example[\"tokens\"]])ï¼Œåˆ™ä¼šæŠŠè¿™ä¸ªåˆ—è¡¨ä¸­æ¯ä¸ªtokençœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œæ˜¾ç„¶ä¸å¯¹ï¼Œ\n",
    "# å¦‚æœåœ¨å¤–é¢åŒ…è£¹ä¸€ä¸ªã€ã€‘ï¼Œåˆ™æ˜¾ç„¶ä¼šæŠ¥é”™ tokenizer([example[\"tokens\"]])\n",
    "# æ‰€ä»¥éœ€è¦ä½¿ç”¨is_split_into_words=True ï¼Œè¿™ä¸ªå‚æ•°ä¼šæŠŠæœ€å°çš„åˆ—è¡¨å…ƒç´ çœ‹ä½œæ˜¯ä¸€ä¸ªæ ·æœ¬\n",
    "# tokenizer(example[\"tokens\"],is_split_into_words=True) # ä¼ å…¥ä¸€ä¸ªæ ·æœ¬ï¼ŒtokenizeræŠŠè¿™ä¸ªtokenåˆ—è¡¨æ ·æœ¬çœ‹ä½œä¸€ä¸ªæ ·æœ¬ï¼Œè€Œä¸”ä¼šå¯¹é‡Œé¢æ¯ä¸ªtokenè¿›è¡Œåˆ‡åˆ†\n",
    "# {'input_ids': [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "# tokenizer([example[\"tokens\"],example[\"tokens\"]],is_split_into_words=True) # ä¼ å…¥ä¸¤ä¸ªæ ·æœ¬ï¼Œtok\n",
    "# {'input_ids': [[101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102], [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "# tokenizer([example[\"tokens\"],example[\"tokens\"]], is_split_into_words=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "example = wnut[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])  # æŠŠidè½¬ä¸ºtokenï¼Œä¸€èˆ¬åœ¨æ˜¯decodeé‡Œé¢çš„æ­¥éª¤\n",
    "print(tokens)\n",
    "print(1)\n",
    "\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é‡æ–°å¯¹é½æ ‡è®°å’Œæ ‡ç­¾ï¼Œå¹¶å°†åºåˆ—æˆªæ–­ä¸ºä¸è¶…è¿‡ DistilBERT çš„æœ€å¤§è¾“å…¥é•¿åº¦\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    è¾“å…¥\n",
    "    tokens ï¼š ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
    "    ner_tagsï¼š [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    è¾“å‡º\n",
    "    input_tokens: ['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n",
    "    input_ids: [101, 1030, 2703, 17122, 2009, 1005, 1055, 1996, 3193, 2013, 2073, 1045, 1005, 1049, 2542, 2005, 2048, 3134, 1012, 3400, 2110, 2311, 1027, 9686, 2497, 1012, 3492, 2919, 4040, 2182, 2197, 3944, 1012, 102]\n",
    "    labels: [-100, 0, -100, -100, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
    "    attention:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        # .word_ids(batch_index=i)ç›´æ¥æŠŠinput_idsä¸­ç¬¬batch_indexçš„å¾—åˆ°å®ƒä»¬çš„å­—åºå·idï¼Œæ¯”å¦‚[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # æŠŠåç¼€è¯ï¼Œæ¯”å¦‚'##sâ€˜éƒ½å˜æˆ-100ï¼Œç„¶åä¿ç•™ç¬¬ä¸€ä¸ªtokençš„label,ç‰¹æ®Šå­—ç¬¦ä¹Ÿæ˜¯-100 , è¿™äº›è¯åœ¨token-clsæ—¶å€™labeléƒ½ä¸º-100\n",
    "        # å…¶ä»–å¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ™ä¿ç•™æ ‡ç­¾ï¼Œæ‰€ä»¥æœ€åçš„æ ‡ç­¾ä¸­é™¤äº†åŸæœ‰bioæ˜ å°„çš„idï¼Œè¿˜æœ‰ç‰¹æ®Šå­—ç¬¦å’Œåç¼€æ˜ å°„çš„-100\n",
    "        # label:[-100, 0, -100, -100, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
    "        # æ ‡ç­¾ä¸­å¯ä»¥æœ‰è´Ÿæ•°ï¼Œè€Œå¦‚æœæ˜¯è´Ÿæ•°çš„è¯ï¼Œåœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶ï¼Œè¿™ä¸ªè´Ÿæ•°å¹¶ä¸ä¼šæ˜ å°„åˆ°æŸä¸ªone-hotå‘é‡ï¼Œä»è€Œä½¿å¾—å¯¹äºlabelæ˜¯è´Ÿæ•°çš„è¿™ä¸ªtokenï¼Œå®ƒçš„æŸå¤±æ˜¯0\n",
    "        # äº¤å‰ç†µæŸå¤±è®¡ç®—æ¯ä¸ªtokenæŸå¤±åè¦é™¤ä»¥æœ‰æŸå¤±çš„æ ·æœ¬æ•°ï¼Œå¦‚æœä¸€ä¸ªtokençš„labelæ˜¯è´Ÿæ•°ï¼Œé‚£ä¹ˆå®ƒä¸ä¼šå‚ä¸è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒä¸ç®—æ ·æœ¬\n",
    "\n",
    "        # å…¶æ¬¡æŠŠæ‰€æœ‰ ## å‰ç¼€çš„tokenå˜ä¸º-100çš„æ ‡ç­¾ï¼Œä¸ºä»€ä¹ˆï¼Ÿ\n",
    "        # å¯ä»¥é™ä½æ¨¡å‹è®­ç»ƒéš¾åº¦ï¼Œå¦‚æœä¸æ˜¯-100ï¼Œåˆ™è¦åŠ å…¥æŸå¤±è®¡ç®—ä¸­ï¼Œé™ä½æ¨¡å‹å­¦ä¹ é€Ÿåº¦,å¢åŠ è¦å­¦ä¹ çš„ç‰¹å¾ä¸ªæ•°ï¼Œæ¨¡å‹å¤æ‚åº¦å¢åŠ ï¼Œ\n",
    "        # è€Œä¸”ç†è®ºä¸Šï¼Œå¸¦##çš„tokenåº”è¯¥å’Œå‰é¢çš„é‚£ä¸ªtokençœ‹æˆä¸€ä¸ªæ•´ä½“ï¼Œæ‰€ä»¥è®¡ç®—å®ƒä»¬ä¸¤ä¸ªåªè¦æœ‰ä¸€ä¸ªç‰¹å¾è¾“å‡ºå°±è¡Œäº†ï¼Œ\n",
    "        # ä¸€èˆ¬ä½¿ç”¨é¦–ä¸ªtokenä½œä¸ºè¿™ä¸€äº›è¯çš„ç‰¹å¾ï¼Œæ‰€ä»¥é€šè¿‡encoderåï¼Œè¿™ä¸ªé¦–ä¸ªtokençš„ç‰¹å¾åº”è¯¥ä¸»è¦åŒ…æ‹¬è¿™ä¸ªè¯çš„ç‰¹å¾ä»¥åŠåé¢å‰ç¼€tokençš„ç‰¹å¾ï¼Œ\n",
    "        # æ‰€ä»¥å°½é‡æŠŠä¸èµ·ä½œç”¨çš„tokenå˜ä¸ºæ— æ„ä¹‰æ ‡ç­¾ï¼ˆç‰¹æ®Šå­—ç¬¦ï¼‰ï¼Œèƒ½ä¸»è§‚åˆå¹¶çš„tokenåªè®¡ç®—ä¸€ä¸ªç‰¹å¾ï¼ˆå‰ç¼€è¯ï¼‰\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "print(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
