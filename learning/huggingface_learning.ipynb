{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcf11dd-a4b4-4125-9f3a-be194d0ff389",
   "metadata": {},
   "source": [
    "# 一.简述\n",
    "> * 使用.from_pretrained导入预训练的config，tokenizer，model,\n",
    "> * 可以使用AutoModel，AutoTokenier，AutoConfig自动载入预训练内容，但建议还是使用确切的模型类，因为可能会有bug\n",
    "> * transformer不是用于构建网络结构的工具，如果想自己重构一个模型，还是要使用pytorch之类的\n",
    "> * single model file 单一模型设计理念,模型前向传播所需的所有代码都在一个且只有一个文件中——称为模型文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be935efe-fa5e-43e2-81c4-759072dfa105",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 二.准备数据\n",
    "> * 数据集的构建，构建的dataset应该是原始文本的，而dataset的处理可以放到后面模型训练用到的时候做\n",
    "> * 这边构建的dataset应该是不变的，可以变的是train和validation，每次都应该可以更新到dataset_dict中，而且应该可以使用cache进行判别是否可以更新\n",
    "> * 具体看datasets_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5502238a-14fd-47a8-b1f3-8888b68c1a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7,8'\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d2df40-13ef-4adf-9559-7cf474371593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyl/disk/poetry_env/algorithms-ai-IgVB5U1f-py3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset glue (/home/zyl/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\",\"cola\",split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e1608-b4ef-42ea-becc-f67741b95234",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 三.构建一个Trainer\n",
    "> * 主要使用Trainer类,传入相关参数\n",
    ">> * model: 模型结构（PreTrainedModel或者torch.nn.Module），包含forward前向传播，模型类初始化，损失的计算。如果没有提供这个参数，model_init必须传入\n",
    ">> * args：模型训练的参数，TrainingArguments\n",
    ">> * data_collator：DataCollator把dataset处理成batch-dataset，会使用default_data_collator，比如DataCollatorWithPadding动态填充\n",
    ">> * train_dataset：训练集，可以是torch.utils.data.Dataset和torch.utils.data.IterableDataset，可以流处理，但是也要在其他地方进行一些配置，用不到的列会被模型-forward自动删除\n",
    ">> * eval_dataset：评估集，可以是torch.utils.data.Dataset或者Dict（torch.utils.data.Dataset），也就是如果使用多个dataset，会在评估的时候前面加上前缀\n",
    ">> * tokenizer：分词PreTrainedTokenizerBase，数据预处理的时候用到，实际包括规范化+预分词+分词，如果提供会自动pad，而且这个独立于模型，方便训练完成后，整体导入模型的时候可以同时导入tokenizer，方便使用\n",
    ">> * model_init：可调用获得模型的函数，Callable[[], PreTrainedModel]，主要用在超参搜索传入不同参数,因为模型初始化会覆盖model\n",
    ">> * compute_metrics：可调用的计算指标的函数，Callable[[EvalPrediction], Dict]输入的是EvalPrediction，输出的是指标字典，自定义指标用到，默认trainer不会在训练时自动评估模型的性能\n",
    ">> * callbacks：可以配置训练的回调函数，List of [`TrainerCallback`],用于自定义训练循环的回调列表，可以在训练过程中自己添加一些操作，也可以删除一些回调，Trainer.remove_callback\n",
    ">> * optimizers：优化器和学习率调度器，Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]默认AdamW和get_linear_schedule_with_warmup\n",
    ">> * preprocess_logits_for_metrics：在评估前处理logits的函数 Callable[[torch.Tensor, torch.Tensor], torch.Tensor]必须传入两个张量logits，labels（默认NONE），所以这个参数在为了配合metrics的时候使用，而且是张量处理，而metrics接收的是array\n",
    "> * Trainer类的相关属性\n",
    ">> * model:核心模型\n",
    ">> * model_wrapped：包裹的模型，比如ddp-model\n",
    ">> * is_model_parallel：是否模型并行（注意：非数据并行，模型的层放在不同gpu上）\n",
    ">> * place_model_on_device:默认Flase（模型并行或deepspeed），是否把模型自动放到device上\n",
    ">> * is_in_train:是否当前的模型处于训练模式，在评估时可能用到这个判断\n",
    "> * 流程：\n",
    ">> * 第一步必须构建模型的结构model（传入一些模型参数，比如层数，只有前向传播），然后需要传入一些模型的训练的超参数args（lr等），这样模型以及基本的训练框架就构建好了\n",
    ">> * 第二步需要传入数据，首先模型的训练集如果较大的时候，可以配置流处理，模型的评估集如果分多个评估集需要弄成一个dict，因为数据集不是batch样的，所以需要data_collator把数据变成batch样的，然后因为数据原始是文本输入，所以要把它变成模型可以接收的input_ids这种形式，用到tokenizer-process，默认使用模型的tokenizer\n",
    ">> * 第三步可以简单自定义一些训练的操作，比如超参搜索model_init，自定义指标compute_metrics，自定义训练callback，自定义优化器和调度器optimizers，自定义logits的处理preprocess_logits_for_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd47570-c6a8-4964-9b66-d43d87b77c9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 构建模型model\n",
    "> * 这里的模型指带参数的checkpoint-model，而不是仅仅模型架构（比如bert结构和bert-base-uncased-checkpoint）\n",
    "> * 模型初始化不仅要初始化模型结构，还要初始化模型参数，如果直接使用Model(config)初始化方法，那就会直接导入模型结构，此时模型的参数是随机初始化的，而from-pretrained方法可以导入预训练模型的参数，即使它传入config，只会做结构上的调整，参数还是预训练模型的参数\n",
    "> * 如果是PreTrainedModel，初始化需要传入一些config（不同模型构建的时候需要不同的模型参数，比如label2id代表的输出层的大小），PreTrainedModel包含forward前向传播过程，forawrd默认返回logits（logits就是最终的全连接层的输出，表示网络最后一层的输出），如果有标签，还会返回loss\n",
    "> * 整个PreTrainedModel实际包括三部分：Configuration（模型构建的配置），backboned（模型架构），model_head（模型要应用的任务构建）\n",
    "> * 有时候一些warning提示预训练权重没有被使用，一些权重随机初始化，正常，比如：bert模型的预训练头被丢弃，取而代之的是随机初始化的分类头\n",
    "> * 构建一个model使用from_pretrained()可以导入hub上的模型（模型默认下载不是下载的原始模型，而是映射后的一些文件，通过这些文件来得到模型，所以把模型从hub上下载下来再使用），也可以导入本地checkpoint，也可以自己从零构建一个模型\n",
    "> * from_pretrained方法--类方法\n",
    ">> * pretrained_model_name_or_path字符串或路径,如果这个值没有，就需要同时传入config和state_dict这两个参数来构建模型,config确定结构，state_dict覆盖参数\n",
    ">> * model_args 模型参数，传入到backbond的，而且这个是位置参数，只在pretrained_model_name_or_path存在时传入模型，做定义结构的作用\n",
    ">> * config 模型配置，如果没有会自动导入\n",
    ">> * state_dict 参数\n",
    ">> * revision hub上的模型版本\n",
    ">> * 出现warning：  The warning *Weights from XXX not initialized from pretrained model* 意味着XXX部分的权重是随机的，不是预训练模型中的，可以通过下游任务进行调参。The warning *Weights from XXX not used in YYY* 意味着在YYY这个模型中的XXX层的参数不被使用，会被丢弃。假设有一个模型A和预训练模型B，如果A比B大几层，那这几层就会报第一个warning，如果A比B少一些层，那这些层就会报第二个warning\n",
    ">> * ignore_mismatched_sizes 默认False，是否忽略上面的两个warning\n",
    ">> * output_loading_info 是否返回一个包含各种信息的字典，默认False\n",
    ">> * 大模型相关的一些参数\n",
    ">>> * low_cpu_mem_usage 加载模型时，尽量不要在 CPU 内存（包括峰值内存）中使用超过 1 倍的模型大小\n",
    ">>> * torch_dtype 模型的参数类型，默认使用原始的参数类型，可以量化使用？？可以训练？？\n",
    ">>> * device_map 设备配置， A map that specifies where each submodule should go，It doesn't need to be refined to each parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the same device.也就是把模型的子模块放到不同机器上,(`str` or `Dict[str, Union[int, str, torch.device]]`,\n",
    ">>> * max_memory 一个内存限制的字典，可以限制cpu的内存和gpu的内存\n",
    ">>> * offload_folder ,offload_state_dict\n",
    ">>> * load_in_8bit 导入模型时使用量化模型，会有性能惩罚，但是还好，注意量化后的模型不能训练\n",
    ">>> * quantization_config量化配置：llm_int8_skip_modules，load_in_8bit_threshold，llm_int8_enable_fp32_cpu_offload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee4578-909c-4eb0-959a-9e91246666d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (1)导入已有的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc818a7-5d87-4ad3-a0b9-4e44dd66c9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "# 注意可以传入模型自身的独特参数以及一些通用模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288dac5-11bd-468c-9fff-a40520cf295b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (2)从零构建一个PreTrainedModel\n",
    "> * Configuration\n",
    ">> * PretrainedConfig，模型的属性，不同模型有不同的属性，比如nlp模型的attention_head,这些属性或者说参数是和模型结构有关的 ，可以修改这些属性来自定义模型架构或者改变已有模型架构（一定程度上）\n",
    ">> * 你的PretrainedConfig的__init__必须接受任何`kwargs, super().__init__(**kwargs)`,那些kwargs需要传递给超类__init__\n",
    ">> * 继承是为了确保您从  Transformers 库中获得所有功能，而其他两个约束来自 PretrainedConfig具有比您设置的字段更多的字段\n",
    "\n",
    "> * Backboned\n",
    ">> * 模型架构，torch.nn.Module,需要传入config，可以自定义forward和模型结构\n",
    ">> * 可以把config载入model，但载入的参数是随机值而不是预训练权重，所以载入预训练要用from_pretrained\n",
    ">> * 如果载入预训练模型时，同时载入config，那就相当于改了预训练模型的部分结构参数\n",
    ">> * forward返回一个dict，里面必须包含logits隐层结果，如果包含loss，那么则可以在trainer中使用\n",
    ">> * 输入原始hidden-state或者features的网络（包含embedding和layers），通常连接到一个接收feature的任务head来做一个预测\n",
    "\n",
    "> * ModelHead\n",
    ">> * 模型任务头，用来匹配不同的任务，Backboned的隐藏状态作为输入传递给模型头以产生最终输出,就是在原始模型基础上改了下forward\n",
    "> * 其他注意\n",
    ">> * 自定义配置,自定义的时候要注意继承，所以我们重新写模型配置是append模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0ac63-463f-4fe7-b52f-702ca11962b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ResnetConfig(PretrainedConfig):\n",
    "    model_type = \"resnet\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_type=\"bottleneck\",\n",
    "        layers: List[int] = [3, 4, 6, 3],\n",
    "        num_classes: int = 1000,\n",
    "        input_channels: int = 3,\n",
    "        cardinality: int = 1,\n",
    "        base_width: int = 64,\n",
    "        stem_width: int = 64,\n",
    "        stem_type: str = \"\",\n",
    "        avg_down: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if block_type not in [\"basic\", \"bottleneck\"]:\n",
    "            raise ValueError(f\"`block_type` must be 'basic' or bottleneck', got {block_type}.\")\n",
    "        if stem_type not in [\"\", \"deep\", \"deep-tiered\"]:\n",
    "            raise ValueError(f\"`stem_type` must be '', 'deep' or 'deep-tiered', got {stem_type}.\")\n",
    "\n",
    "        self.block_type = block_type\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes\n",
    "        self.input_channels = input_channels\n",
    "        self.cardinality = cardinality\n",
    "        self.base_width = base_width\n",
    "        self.stem_width = stem_width\n",
    "        self.stem_type = stem_type\n",
    "        self.avg_down = avg_down\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "resnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\n",
    "resnet50d_config.save_pretrained(\"custom-resnet\")\n",
    "my_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n",
    "\n",
    "\n",
    "# from transformers import DistilBertConfig\n",
    "#\n",
    "# config = DistilBertConfig()\n",
    "# my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4)\n",
    "# my_config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n",
    "# my_config.save_pretrained(save_directory=\"./your_model_save_path\")  # 保存 JSON 文件\n",
    "# my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\") # 使用json配置文件\n",
    "# model = DistilBertModel(my_config)  # 这将创建一个具有随机值而不是预训练权重的模型\n",
    "\n",
    "# 使用\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04843c10-7492-4b0a-9aec-59933c6d5d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from timm.models.resnet import BasicBlock, Bottleneck, ResNet\n",
    "from .configuration_resnet import ResnetConfig\n",
    "\n",
    "\n",
    "BLOCK_MAPPING = {\"basic\": BasicBlock, \"bottleneck\": Bottleneck}\n",
    "\n",
    "\n",
    "class ResnetModel(PreTrainedModel):\n",
    "    config_class = ResnetConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        block_layer = BLOCK_MAPPING[config.block_type]\n",
    "        self.model = ResNet(\n",
    "            block_layer,\n",
    "            config.layers,\n",
    "            num_classes=config.num_classes,\n",
    "            in_chans=config.input_channels,\n",
    "            cardinality=config.cardinality,\n",
    "            base_width=config.base_width,\n",
    "            stem_width=config.stem_width,\n",
    "            stem_type=config.stem_type,\n",
    "            avg_down=config.avg_down,\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return self.model.forward_features(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da230dbf-0f80-4aef-9ca1-f729f098a74e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ResnetModelForImageClassification(PreTrainedModel):\n",
    "    config_class = ResnetConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        block_layer = BLOCK_MAPPING[config.block_type]\n",
    "        self.model = ResNet(\n",
    "            block_layer,\n",
    "            config.layers,\n",
    "            num_classes=config.num_classes,\n",
    "            in_chans=config.input_channels,\n",
    "            cardinality=config.cardinality,\n",
    "            base_width=config.base_width,\n",
    "            stem_width=config.stem_width,\n",
    "            stem_type=config.stem_type,\n",
    "            avg_down=config.avg_down,\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor, labels=None):\n",
    "        logits = self.model(tensor)\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "\n",
    "resnet50d = ResnetModelForImageClassification(resnet50d_config)\n",
    "\n",
    "# 导入参数\n",
    "# import timm\n",
    "\n",
    "# pretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\n",
    "# resnet50d.model.load_state_dict(pretrained_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045e54a-b6ce-42f8-960a-22507b1f3ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. 构建数据的处理器\n",
    "> * 主要是tokenizer，一般使用模型对应的tokenizer，主要是把原始文本转为模型能够接收的输入\n",
    "> * 这里的tokenizer不仅仅是tokenizer，还包括normalizer，pre-tokenizer,tokenizer,postprocess\n",
    "> * PreTrainedTokenizer：分词器的 Python 实现。PreTrainedTokenizerFast：基于 Rust 的Tokenizers库的分词器。fast tokenizer 还提供了额外的方法，例如将标记映射到其原始单词或字符的偏移映射,支持编码和解码、添加新标记和管理特殊标记等常用方法,但并非每个模型都支持快速分词器\n",
    "> * 记住，自定义分词器生成的词汇与预训练模型的分词器生成的词汇不同,如果您使用的是预训练模型，则需要使用预训练模型的词汇表，否则输入将没有意义\n",
    "> * 可以导入一个已有的tokenizer，也可以自定义训练一个tokenizer（见tokenizers_learning.ipynb）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318d23fb-ec09-4563-a9d7-886335db48f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 导入模型分词器\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dabbc-a7ab-448e-b108-a4dbe9201283",
   "metadata": {},
   "source": [
    "## 3. 处理模型需要的数据\n",
    "> *要将标签映射到labels字段，这样做是因为使用Trainer进行模型训练时，会自动找到labels字段作为标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1194af85-e155-4601-bd95-c1369fd4c4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.80ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 68.99ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset,validation_dataset = dataset.train_test_split(test_size=0.2).values()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, keep_in_memory=True, batch_size=2000)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True, keep_in_memory=True, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729adcae-e2b3-45f7-bb7b-cb1fba339432",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. 配置模型的data-collator\n",
    "> * 默认 DataCollatorWithPadding把label列换成了labels，把label_ids列换成了labels\n",
    "> * 需要pad操作，动态填充效率更高\n",
    "> * 不同任务可选很多datacollator，比如DataCollatorForTokenClassification\n",
    "> * 自定义data-collator的输入是features: 是一个batch大小的list，每个element是一个字典，key为模型接收的key，比如'labels', 'input_ids','token_type_ids', 'attention_mask'，输出的是模型接受的batch数据字典，每个value都是一个tensor，而且可以做padding什么的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54714112-e1b9-40e3-ac51-44e942704109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 准备data-collator, data-collector时使用padding，返回pt格式\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator_padding = DataCollatorWithPadding(tokenizer=tokenizer,\n",
    "                                               padding='max_length',\n",
    "                                               max_length=tokenizer.model_max_length,\n",
    "                                               pad_to_multiple_of=None,\n",
    "                                               return_tensors=\"pt\",\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3232d4-3e46-4625-a014-6e0313109093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以自定义data-collator\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "class Mycollator(DataCollatorWithPadding):\n",
    "    def __init__(self,tokenizer,):\n",
    "        super(Mycollator, self).__init__(tokenizer=tokenizer)\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        # if \"label\" in batch:\n",
    "        #     batch[\"labels\"] = batch[\"label\"]\n",
    "        #     del batch[\"label\"]\n",
    "        # if \"label_ids\" in batch:\n",
    "        #     batch[\"labels\"] = batch[\"label_ids\"]\n",
    "        #     del batch[\"label_ids\"]\n",
    "        return batch\n",
    "\n",
    "data_collator = Mycollator(tokenizer=tokenizer)  # 不将label删掉变成labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585876c4-2569-425d-aff1-d32442174108",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.配置模型训练参数\n",
    "> * 数据相关的参数\n",
    ">> * 'dataloader_drop_last': False,  # 是否删除最后一个不完整的batch\n",
    ">> * 'dataloader_num_workers': 0,  # 多进程处理数据，载入数据进程数，0默认1个进程，只加载到主进程，告诉dataloader要创建多少个子进程进行数据加载，和cpu有关，和gpu无关\n",
    ">> * 'remove_unused_columns': True,  # 自动取出多余的列\n",
    ">> * 'label_names':  # label相关的列的名称，List[str]，默认[\"label\"],如果是问答则是[\"start_positions\", \"end_positions\"]\n",
    ">> * 'disable_tqdm': # 是否关闭进度和metric\n",
    ">> * 'ignore_data_skip':False  # 恢复训练时，是否跳过epochs和batches，让数据同时加载阶段和之前的训练一样。 如果设置为“True”，训练将开始得更快（因为跳过的步骤可能需要很长时间）但不会产生与中断训练相同的结果\n",
    ">> * 'group_by_length': False,  # 默认False, 动态padding时使用，处理数据的时候把长度相同的放在一起，加速推理，训练时尽量不要用，否则短的在一起，长的在一起\n",
    ">> * 'dataloader_pin_memory': True,  # 是否固定数据内存 ，默认True\n",
    ">> * 其他：'past_index'\n",
    "\n",
    "> * 训练相关的参数\n",
    ">> * 'num_train_epochs': 3,  # 训练epoch\n",
    ">> * 'max_steps': -1,  # 总的迭代次数，如果设置会覆盖epoch\n",
    ">> * 'per_device_train_batch_size': 8,  # 每个设备（可能是GPU）的训练batch大小\n",
    ">> * 'gradient_accumulation_steps': 1,  # 梯度累计step数\n",
    ">> * 'weight_decay': 0,  # 权重衰减，L2正则化\n",
    ">> * 'max_grad_norm': 1,  # 梯度截断，控制梯度膨胀\n",
    ">> * 'no_cuda': False,  # 是否使用GPU\n",
    ">> * 'seed': 42,  # 训练时的种子\n",
    ">> * 'data_seed': # 数据种子，如果不设置就默认为训练的种子\n",
    ">> * full_determinism # 可以确保重现分布式训练的结果，需要配合args.need，设置会True则会固定分布式的一些配置，而不是让他随机\n",
    ">> * gradient_checkpointing #  如果为 True，则使用梯度检查点以较慢的反向传递为代价来节省内存。\n",
    ">> * fp16 : 混合精度计算\n",
    ">> * resume_from_checkpoint： 从检查点恢复训练，第一种是不使用overwrite_output_dir，会使用之前的output_dir，第二种是使用新的output_dir，此时可overwrite_output_dir，resume_from_checkpoint=True它将从最新的检查点恢复训练，resume_from_checkpoint=checkpoint_dir这将从传递的目录中的特定检查点恢复训练。\n",
    ">> * sharded_ddp分布式计算，可以用\n",
    ">> * fsdp 一种分片分布式计算\n",
    ">> * 其他：'deepspeed',auto_find_batch_size，torchdynamo\n",
    "\n",
    "> * 优化器和学习器相关的参数\n",
    ">> * 'optim': 'adamw_hf',\n",
    ">> * 'optim_args':  # 参数\n",
    ">> * 'learning_rate': 5e-5,  # lr,默认使用AdamW优化器\n",
    ">> * 'lr_scheduler_type': 'linear',  # 学习率优化\n",
    ">> * 'adam_beta1': 0.9,\n",
    ">> * 'adam_beta2': 0.999,\n",
    ">> * 'adam_epsilon': 1e-8,\n",
    ">> * 'warmup_ratio': 0.0,  # 预热的整个step的比例\n",
    ">> * 'warmup_steps': 0,  # 预热的步数，如果设置，会覆盖比例\n",
    ">> * 其他：\n",
    "\n",
    "> * 评估相关参数\n",
    ">> * 'evaluation_strategy': 'steps',  # 默认若干步评估一次，同时log输出一次\n",
    ">> * 'eval_steps':  # 评估的间隔step数，如果不设置，就会和logging_steps一样\n",
    ">> * 'per_device_eval_batch_size': 8,  # 每个设备（可能是GPU）的评估batch大小\n",
    ">> * 'eval_accumulation_steps': , # 在把结果放到cpu上前，评估累计的次数，不设置就评估整个评估集\n",
    ">> * 'eval_delay': # 评估的延迟步数，一般不用\n",
    ">> * 'load_best_model_at_end': False,  # 最后载入最好的指标的那次模型，如果设置为True，save_strategy要和evaluation_strategy一样，`save_steps` 必须是 `eval_steps`的整数倍,是否在训练结束时加载在训练中找到的最佳模型，注意这个默认是根据loss来的，默认false，如果设置为True，那么所有训练完成后，会把最好结果的模型保存下来，因为训练时只会记录最好模型是哪个，而不会使用，所以最后\n",
    ">> * 'metric_for_best_model':, # 评估保存的标准是什么，默认loss,训练时默认在前面加上前缀 'eval_'\n",
    ">> * 'greater_is_better':True # 模型指标的选择,配合metric_for_best_model使用\n",
    ">> * 'include_inputs_for_metrics': False,  # 是否把输入传入到metric计算，如果自定义的metric需要input，则需要设置这个为True\n",
    ">> * 其他：\n",
    "\n",
    "> * log相关参数\n",
    ">> * 'logging_dir': './output_dir/runs',  # log日志保存在哪\n",
    ">> * 'logging_strategy': 'steps',  # 日志的保存策略，默认steps\n",
    ">> * 'logging_steps': 500,  # log策略是steps时的步骤数\n",
    ">> * 'log_level': 'passive',  # 主进程的log的等级\n",
    ">> * 'log_level_replica': 'warning',  # 重复log的等级\n",
    ">> * 'log_on_each_node': True,  # 分布式训练是否log每个节点\n",
    ">> * 'logging_first_step': False,  # 是否评估和log开始时的结果\n",
    ">> * 'logging_nan_inf_filter': True,  # 是否过滤空值和无穷小的loss\n",
    ">> * 'report_to':  上传到哪\n",
    ">> * 'run_name':  名称\n",
    ">> * 'skip_memory_metrics': True,  # 是否跳过内存使用分析报告，默认跳过，设置为False会出一个内存使用分析，会使得训练和评估变慢，应该只在inference测试时用，\n",
    ">> * 'push_to_hub': False,\n",
    ">> * 其他\n",
    "\n",
    "> * 保存模型相关参数\n",
    ">> * output_dir 必传参数，模型保存路径\n",
    ">> * 'overwrite_output_dir': False,  # 是否覆写输出的路径\n",
    ">> * 'save_strategy': \"steps\",  # 保存策略，默认steps\n",
    ">> * 'save_steps': 500,  # steps时保存的步骤\n",
    ">> * 'save_total_limit': 2,  # 最大存储的模型数,为了缩小最大的存储模型的个数\n",
    ">> * 'save_on_each_node': False,  # 多节点训练时，是否在每个节点上存储模型，还是只在主节点上存储\n",
    ">> * 其他\n",
    "\n",
    "> * 推理相关\n",
    ">> * \"prediction_loss_only\": False,  # 是否预测时只输出损失\n",
    ">> * 'jit_mode_eval': False,  # 是否使用jit进行inference\n",
    ">> * \n",
    "> * 其他：label_smoothing_factor ，do_train，do_eval，do_predict，debug,use_legacy_prediction_loop\n",
    "> * 必传参数output_dir,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511399aa-40c8-49e2-b573-8ab0fd5e8a22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # do_train=True,\n",
    "    # do_eval=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,\n",
    "    label_names=['labels'],\n",
    "    logging_steps=20,\n",
    "    save_total_limit= 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293496eb-7038-4955-b6bd-a3479809d369",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. 自定义一些其他操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96478af-e776-4a57-9e34-9eb1571724fd",
   "metadata": {},
   "source": [
    "### 评估指标\n",
    "> * 可调用的函数，输入的是EvalPrediction，输出的是指标字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b5c960-ded4-4e35-8ac6-bd90c73829bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    r = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63864ad1-3381-4367-be7d-e9991544b26f",
   "metadata": {},
   "source": [
    "## 7.构建Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd72fa65-7f85-4a17-a7e6-d0e9ecafeb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.select(range(200)),\n",
    "    eval_dataset=validation_dataset.select(range(100)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_padding,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40764e93-d7f2-4daa-ab5d-85bff15b9bca",
   "metadata": {},
   "source": [
    "## 8.Trainer的使用\n",
    "> * train() 训练\n",
    ">> * resume_from_checkpoint:从检查点继续训练，会导入model/optimizer/scheduler，默认False,可以直接输入路径\n",
    ">> * trial:超参数搜索的试验数\n",
    ">> * ignore_keys_for_eval：模型输出中的键列表（如果它是字典），在训练期间收集预测以进行评估时应忽略这些键。\n",
    "\n",
    "> * evaluate() 评估数据集，返回EvalLoopOutput\n",
    ">> * eval_dataset评估集\n",
    ">> * ignore_keys可以忽略的指标\n",
    ">> * metric_key_prefix，每个评估指标都会加'eval_'前缀\n",
    ">> * 输入的是评估的预测结果，eval_pred:transformers.trainer_utils.EvalPrediction有predictions，label_ids，inputs三个属性,其中predictions和label_ids是array,而且predictions是logits\n",
    "\n",
    "> * prediction_step 模型单步预测\n",
    ">> * 输入model，inputs，prediction_loss_only,ignore_keys\n",
    ">> * 返回(loss, logits, labels)\n",
    ">> * evaluation_loop 用到prediction_step\n",
    "\n",
    "> * create_optimizer_and_scheduler 创建优化器和调度器\n",
    "> * get_train_dataloader\n",
    "> * training_step\n",
    "> * compute_loss(self, model, inputs, return_outputs=False)\n",
    "\n",
    "> * 其他：可以自定义继承Trainer，很多方法都可以重写，模型预测的损失只会有一个值，但如果用DataParallel之类的模型，那么损失会是多个，每个是每个gpu的损失，然后gather组合起来这些，训练是每轮训练，然后每个更新step，然后每个子step\n",
    "> * 默认使用ddp进行训练，取决于使用的os-gpu-env，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371cf78-64f8-4a37-bd62-db9d09af3375",
   "metadata": {},
   "source": [
    "# 四.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904db7a-6b4c-42f2-9ed1-4a63477cf2a7",
   "metadata": {},
   "source": [
    "## 1. 可以直接训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7c3b833-950d-42aa-a908-521da8af1b76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/zyl/disk/poetry_env/algorithms-ai-IgVB5U1f-py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 200\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n",
      "  Number of trainable parameters = 109483778\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/home/zyl/disk/poetry_env/algorithms-ai-IgVB5U1f-py3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.674351</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.655514</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.679990</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to my_awesome_model/checkpoint-13\n",
      "Configuration saved in my_awesome_model/checkpoint-13/config.json\n",
      "Model weights saved in my_awesome_model/checkpoint-13/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/checkpoint-13/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/checkpoint-13/special_tokens_map.json\n",
      "/home/zyl/disk/poetry_env/algorithms-ai-IgVB5U1f-py3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to my_awesome_model/checkpoint-26\n",
      "Configuration saved in my_awesome_model/checkpoint-26/config.json\n",
      "Model weights saved in my_awesome_model/checkpoint-26/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/checkpoint-26/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/checkpoint-26/special_tokens_map.json\n",
      "/home/zyl/disk/poetry_env/algorithms-ai-IgVB5U1f-py3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to my_awesome_model/checkpoint-39\n",
      "Configuration saved in my_awesome_model/checkpoint-39/config.json\n",
      "Model weights saved in my_awesome_model/checkpoint-39/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_model/checkpoint-39/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_model/checkpoint-39/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_model/checkpoint-13] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from my_awesome_model/checkpoint-26 (score: 0.68).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=0.29291621232644105, metrics={'train_runtime': 23.4246, 'train_samples_per_second': 25.614, 'train_steps_per_second': 1.665, 'total_flos': 157866633216000.0, 'train_loss': 0.29291621232644105, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bc4c6-2447-4396-b5a0-48b3ac9de878",
   "metadata": {},
   "source": [
    "## 1.可以自定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc300c-3ac6-42cf-998d-336a6249b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义训练\n",
    "import torch\n",
    "torch.cuda.empty_cache()  # 释放一些存储\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 数据加载器\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "\n",
    "# 优化器，由于优化一些层的参数\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# scheduler调度器可以指定在每个step更新学习率，但调度器不仅仅可以是学习率调度器\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 训练\n",
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # 梯度计算\n",
    "\n",
    "        optimizer.step()  # 优化器根据梯度更新参数，所以优化器指参数更新方法\n",
    "        lr_scheduler.step()  # 更新超参数：比如lr，也可以其他\n",
    "        optimizer.zero_grad()  # 如果不将梯度归0，则它将添加到下一步的梯度中\n",
    "        progress_bar.update(1)\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddca26c-57c3-4780-a021-fa3bc5184cc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Method\tSpeed\tMemory\n",
    "Gradient accumulation\tNo\tYes\n",
    "Gradient checkpointing\tNo\tYes\n",
    "Mixed precision training\tYes\t(No)\n",
    "Batch size\tYes\tYes\n",
    "Optimizer choice\tYes\tYes\n",
    "DataLoader\tYes\tNo\n",
    "DeepSpeed Zero\tNo\tYes## 2.多GPU训练\n",
    "> * 设置要使用的gpu环境变量：CUDA_VISIBLE_DEVICES='0,1,2'\n",
    "> * 应该使用ddp训练：`python -m torch.distributed.launch --nproc_per_node 2 run_train.py`或者torchrun:`torchrun --nproc_per_node=2 run_train.py`\n",
    "> * 默认使用DP进行训练，它的速度和DDP的速度相差巨大(若干倍，取决于使用的gpu数)\n",
    "> * 使用DDP的时候wandb的环境要在主进程上，使用`os.environ[\"WANDB_PROJECT\"] = \"test\"`而不是wandb.init，否则可能创建多个wandb-log\n",
    "> * P2P训练，要设置环境变量：export NCCL_P2P_DISABLE=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821eda01-13f4-49a0-94ff-6b9ce1305719",
   "metadata": {},
   "outputs": [],
   "source": [
    "Method\tSpeed\tMemory\n",
    "Gradient accumulation\tNo\tYes\n",
    "Gradient checkpointing\tNo\tYes\n",
    "Mixed precision training\tYes\t(No)\n",
    "Batch size\tYes\tYes\n",
    "Optimizer choice\tYes\tYes\n",
    "DataLoader\tYes\tNo\n",
    "DeepSpeed Zero\tNo\tYes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e910a-84f1-432c-95f6-625df88d8a23",
   "metadata": {},
   "source": [
    "## 3.accelerate加速训练\n",
    "> * Accelerator将自动检测您的分布式设置类型并初始化所有必要的训练组件。您不需要明确地将您的模型放置在设备上\n",
    "> * 使用Accelerator类来加速训练，需要传入一些配置\n",
    "> * 不再需要model放在gpu这种操作.不再需要数据放在gpu这种操作,使用accelerator.backward(loss)来替代loss.backward()\n",
    "> * accelerate config ：create and save a configuration file\n",
    "> * accelerate launch train.py ：  launch your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1f7cf-ddc9-4c20-8c92-c0eded459e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# # 所有相关的训练对象传递给prepare方法\n",
    "# train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "#     train_dataloader, eval_dataloader, model, optimizer\n",
    "# )\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloader:\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         accelerator.backward(loss)\n",
    "#\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         progress_bar.update(1)\n",
    "#\n",
    "# + from accelerate import Accelerator\n",
    "#   from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "#\n",
    "# + accelerator = Accelerator()\n",
    "#\n",
    "#   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "#   optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "#\n",
    "# - device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# - model.to(device)\n",
    "#\n",
    "# + train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "# +     train_dataloader, eval_dataloader, model, optimizer\n",
    "# + )\n",
    "#\n",
    "#   num_epochs = 3\n",
    "#   num_training_steps = num_epochs * len(train_dataloader)\n",
    "#   lr_scheduler = get_scheduler(\n",
    "#       \"linear\",\n",
    "#       optimizer=optimizer,\n",
    "#       num_warmup_steps=0,\n",
    "#       num_training_steps=num_training_steps\n",
    "#   )\n",
    "#\n",
    "#   progress_bar = tqdm(range(num_training_steps))\n",
    "#\n",
    "#   model.train()\n",
    "#   for epoch in range(num_epochs):\n",
    "#       for batch in train_dataloader:\n",
    "# -         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#           outputs = model(**batch)\n",
    "#           loss = outputs.loss\n",
    "# -         loss.backward()\n",
    "# +         accelerator.backward(loss)\n",
    "#\n",
    "#           optimizer.step()\n",
    "#           lr_scheduler.step()\n",
    "#           optimizer.zero_grad()\n",
    "#           progress_bar.update(1)\n",
    "\n",
    "\n",
    "accelerate 脚本训练\n",
    "accelerate config\n",
    "accelerate config\n",
    "accelerate launch run_summarization_no_trainer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d578e0-3c41-4269-90d1-b84756fd95f4",
   "metadata": {},
   "source": [
    "# 五.使用训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9e894cb-795b-4010-b0e1-6715ee82feef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = './my_awesome_model/checkpoint-39'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50bb3a-4f86-4d36-95fb-332558c833e5",
   "metadata": {},
   "source": [
    "## 1.单独使用\n",
    "> 需要自己构建tokenizer，数据处理，和模型前向传播，不是很推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15966e25-5ecc-4257-ae13-07014f91def7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./my_awesome_model/checkpoint-39/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./my_awesome_model/checkpoint-39/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./my_awesome_model/checkpoint-39.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "use_model = BertForSequenceClassification.from_pretrained(checkpoint)\n",
    "# for epoch in range(num_epochs):\n",
    "#       for batch in train_dataloader:\n",
    "# -         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#           outputs = model(**batch)\n",
    "#           loss = outputs.loss\n",
    "# -         loss.backward()\n",
    "# +         accelerator.backward(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd51eb-18b9-4963-9594-194a8c65a34b",
   "metadata": {},
   "source": [
    "## 2.pipeline\n",
    "> * 可以使用已有的pipeline，也可以根据需求自己构建pipeline\n",
    "> * 管道pipeline主要用于inference，包含预处理，后处理和模型推理，相当于api,使用时，直接调用，可以输入list\n",
    "> * 注意：管道的批处理并不会更好，因为如果一个批次里面出现一个长句子，那么整个批次的token大小就会是这个长token，如果数据很大，可以分块进行处理\n",
    "> * 批处理只有在特定情况下才能完全发挥作用，比如gpu很多很大\n",
    "> * 可以使用accelerate加速大模型的推理\n",
    "> * 有preprocess，forward，postprocess可以分开来使用，所以有多个前向传播或复杂情况可以用这个\n",
    "> * 这个pipeline仅支持单gpu预测,更多的是偏向在在线流处理，如果有已确定的很多数据要进行预测，还是要使用trainer-predict-多gpu预测\n",
    "> * 如果模型对于单个 GPU 来说太大，可以设置device_map=\"auto\"允许 Accelerate自动确定如何加载和存储模型权重,device和device_map不共存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa5d2810-6314-480d-a2a7-c535cc6240bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./my_awesome_model/checkpoint-39/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./my_awesome_model/checkpoint-39\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ./my_awesome_model/checkpoint-39/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./my_awesome_model/checkpoint-39\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./my_awesome_model/checkpoint-39/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./my_awesome_model/checkpoint-39.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification',checkpoint)\n",
    "generator = pipeline(model=\"openai/whisper-large\", device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5215f0-0ae1-4c2a-81cf-3bae6f964d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果处理很多数据，使用数据迭代器\n",
    "def data():\n",
    "    for i in range(1000):\n",
    "        yield f\"My example {i}\"\n",
    "\n",
    "\n",
    "pipe = pipeline(model=\"gpt2\", device=0)\n",
    "generated_characters = 0\n",
    "for out in pipe(data()):\n",
    "    generated_characters += len(out[0][\"generated_text\"])\n",
    "\n",
    "# 使用大模型\n",
    "# pip install accelerate\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5459d4c3-7181-4bd1-8f18-b4cdd8a900b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.8845857381820679}]\n",
      "{'input_ids': tensor([[  101,  1996, 11279,  8469,  1996,  9478,  3154,  1997,  1996,  5749,\n",
      "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ModelOutput([('logits', tensor([[-1.2044,  0.8322]]))])\n",
      "{'label': 'LABEL_1', 'score': 0.8845857381820679}\n"
     ]
    }
   ],
   "source": [
    "print(classifier(dataset[0]['sentence']))\n",
    "print(classifier.preprocess(dataset[0]['sentence']))\n",
    "print(classifier.forward(classifier.preprocess(dataset[0]['sentence'])))\n",
    "print(classifier.postprocess(classifier.forward(classifier.preprocess(dataset[0]['sentence']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc5be1-0586-4d2d-8845-83c27919dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加速accelerate\n",
    "# pip install accelerate bitsandbytes\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n",
    "# output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ee593-a23c-40e7-8b1f-f296b9e51fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. 自己构建pipeline\n",
    "> * 这个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb46b07-d8d7-4652-86ec-049e5eafc921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        # 这个函数为了让用户可以随时传递任何参数\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "\n",
    "        postprocess_kwargs = {}\n",
    "        if \"top_k\" in kwargs:\n",
    "            postprocess_kwargs[\"top_k\"] = kwargs[\"top_k\"]\n",
    "        return preprocess_kwargs, {}, postprocess_kwargs\n",
    "    \n",
    "    \n",
    "    def preprocess(self, inputs, maybe_arg=2):\n",
    "        model_input = Tensor(inputs[\"input_ids\"])\n",
    "        return {\"model_input\": model_input}\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # model_inputs == {\"model_input\": model_input}\n",
    "        outputs = self.model(**model_inputs)\n",
    "        # Maybe {\"logits\": Tensor(...)}\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        best_class = model_outputs[\"logits\"].softmax(-1)\n",
    "        return best_class\n",
    "\n",
    "my_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\n",
    "my_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d1e80-2a93-4b28-a910-7da6ca99b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\")\n",
    "\n",
    "\n",
    "def data():\n",
    "    while True:\n",
    "        # This could come from a dataset, a database, a queue or HTTP request\n",
    "        # in a server\n",
    "        # Caveat: because this is iterative, you cannot use `num_workers > 1` variable\n",
    "        # to use multiple threads to preprocess data. You can still have 1 thread that\n",
    "        # does the preprocessing while the main runs the big inference\n",
    "        yield \"This is a test\"\n",
    "\n",
    "\n",
    "for out in pipe(data()):\n",
    "    print(out)\n",
    "    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n",
    "    # {\"text\": ....}\n",
    "    # ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e5074-9e66-458e-825e-15aab36da42e",
   "metadata": {},
   "source": [
    "# 六.其他\n",
    "deepspeed\n",
    "\n",
    "\n",
    "flexflow\n",
    "\n",
    "\n",
    "benchmarkert 需要一个计算时间和内存使用的统计追踪功能\n",
    "\n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.device(\"cuda:1,3\" if torch.cuda.is_available() else \"cpu\")  ## specify the GPU id's, GPU id's start from 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7d432-5d71-44cf-9ade-1c2072861a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "冻结层\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "     print(name, param.requires_grad)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith(\"...\"): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "要冻结层就要从原始结构中进行冻结，就是找到整个模型的基准模型中的层进行冻结，可以指定层\n",
    "还有，要注意如果冻结了层，但是在trian模式的话，有dropout，所以有输出不同的情况，所以要验证这个冻结层到底有没有冻结，请在eval模式进行\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52940b82-e7c8-4934-bb09-841d6211d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ab0a1-87d8-4426-9abc-14ecadcbe2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "当批量大小和输入/输出神经元计数可以被某个数字(8)整除时，一个人获得最有效的性能，这个数字通常从 8 开始，但也可以更高。该数字根据所使用的特定硬件和模型的数据类型而有很大差异。\n",
    "\n",
    "选择词汇量为 8 的倍数可显着提高性能\n",
    "batch-size大小为 8 的倍数\n",
    "batch-size 例如，对于 fp16，建议使用 8 的倍数，但在 A100 上为 64\n",
    "\n",
    "如果我们想以 64 的批量大小进行训练，我们不应该使用per_device_train_batch_size=1andgradient_accumulation_steps=64而是使用per_device_train_batch_size=4和 ，gradient_accumulation_steps=16它具有相同的有效批量大小，同时可以更好地利用可用的 GPU 资源。\n",
    "\n",
    "所以训练时 先确定要使用batch-size大小（8的倍数），若太大，则使用梯度累积\n",
    "\n",
    "即使我们将批量大小设置为 1 并使用梯度累积，我们在处理大型模型时仍然会耗尽内存。为了在反向传播过程中计算梯度，通常会保存来自正向传播的所有激活。这会产生很大的内存开销。或者，可以忘记前向传播过程中的所有激活，并在后向传播过程中根据需要重新计算它们。然而，这会增加大量的计算开销并减慢训练速度。\n",
    "\n",
    "梯度检查点在两种方法之间取得了折衷，并在整个计算图中保存了策略性选择的激活，因此只需为梯度重新计算一小部分激活。请参阅这篇解释梯度检查点背后思想的精彩文章。\n",
    "\n",
    "使用梯度累积的时候使用梯度检查点，这节省了更多内存，但同时训练速度变慢了。一般的经验法则是，梯度检查点会使训练速度减慢约 20%。让我们看看另一种可以恢复速度的方法：混合精度训练\n",
    "\n",
    "\n",
    "混合精度训练的想法是，并非所有变量都需要以完整（32 位）浮点精度存储。如果我们可以降低变量的精度，它们的计算就会更快。以下是影响内存使用和吞吐量的常用浮点数据类型的选择：\n",
    "\n",
    "fp32 ( float32)\n",
    "fp16 ( float16)\n",
    "bf16 ( bfloat16)\n",
    "tf32（CUDA 内部数据类型）\n",
    "\n",
    "混合精度训练的想法是，并非所有变量都需要以完整（32 位）浮点精度存储。如果我们可以降低变量的精度，它们的计算就会更快。主要优势来自于以一半（16 位）精度保存激活。尽管梯度也是以半精度计算的，但它们会在优化​​步骤中转换回全精度，因此此处不节省内存。由于模型以 16 位和 32 位精度存在于 GPU 上，因此可以使用更多 GPU 内存（GPU 上原始模型的 1.5 倍），尤其是对于小批量。由于有些计算是全精度的，有些是半精度的，因此这种方法也称为混合精度训练。启用混合精度训练也只是将fp16标志设置为True:\n",
    "    \n",
    "    如果您可以使用 Ampere 或更新的硬件，则可以使用 bf16 进行训练和评估。虽然 bf16 的精度比 fp16 差，但它的动态范围要大得多。因此，如果过去您在训练模型时遇到溢出问题，bf16 将在大多数情况下防止这种情况发生。请记住，在 fp16 中，您可以拥有的最大数字是“65535”，超过该数字的任何数字都会溢出。bf16 数字可以大到“3.39e+38”（！），这与 fp32 大致相同——因为两者都有 8 位用于数值范围。\n",
    "    \n",
    "    用于训练 Transformer 模型的最常见优化器是 Adam 或 AdamW（具有权重衰减的 Adam）。Adam 通过存储先前梯度的滚动平均值来实现良好的收敛，但是，这会增加模型参数数量级的额外内存占用。对此的一种补救措施是使用替代优化器，例如 Adafactor，它对某些模型效果很好，但通常存在不稳定问题。\n",
    "    \n",
    "    Accelerate加速\n",
    "    \n",
    "    \n",
    "    \n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "accelerator = Accelerator(fp16=training_args.fp16)\n",
    "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(dataloader, start=1):\n",
    "    loss = model(**batch).loss\n",
    "    loss = loss / training_args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % training_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "达到高训练速度的重要要求之一是能够以 GPU 可以处理的最大速度提供数据。默认情况下，一切都发生在主进程中，它可能无法足够快地从磁盘读取数据，从而造成瓶颈，导致 GPU 利用率不足。\n",
    "\n",
    "DataLoader(pin_memory=True, ...)这确保数据被预加载到 CPU 上的固定内存中，并且通常会导致从 CPU 到 GPU 内存的传输速度更快。\n",
    "DataLoader(num_workers=4, ...)- 生成多个工作人员以更快地预加载数据 - 在训练期间观察 GPU 利用率统计数据，如果距离 100% 还很远，则尝试增加工作人员数量。当然，问题可能出在其他地方，因此大量的 worker 不一定会带来更好的性能。\n",
    "\n",
    "\n",
    "当我们训练模型时，我们希望同时优化两个方面：\n",
    "\n",
    "数据吞吐量/训练时间\n",
    "模型性能\n",
    "我们已经看到每种方法都会改变内存使用和吞吐量。一般来说，我们希望最大化吞吐量（样本/秒）以最小化训练成本。这通常是通过尽可能多地利用 GPU 并将 GPU 内存填充到极限来实现的。例如，如前所述，只有当我们想要使用超过 GPU 内存大小时的批量大小时，我们才使用梯度累积。如果所需的批量大小适合内存，则没有理由应用只会减慢训练速度的梯度累积。\n",
    "\n",
    "第二个目标是模型性能。仅仅因为我们可以并不意味着我们应该使用大批量。作为超参数调整的一部分，您应该确定哪个批量大小会产生最佳结果，然后相应地优化吞吐量。\n",
    "\n",
    "DataParallel (DP) - 相同的设置被复制多次，并且每次都被馈送数据的一部分。处理是并行完成的，所有设置在每个训练步骤结束时同步。\n",
    "TensorParallel (TP) - 每个张量被分成多个块，因此不是让整个张量驻留在单个 gpu 上，而是张量的每个分片都驻留在其指定的 gpu 上。在处理过程中，每个分片在不同的 GPU 上分别并行处理，结果在步骤结束时同步。这就是所谓的水平并行性，因为拆分发生在水平面上。\n",
    "PipelineParallel (PP) - 模型在多个 GPU 上垂直（层级）拆分，因此只有一个或多个模型层位于单个 gpu 上。每个 gpu 并行处理管道的不同阶段，并处理一小部分批处理。\n",
    "零冗余优化器 (ZeRO) - 也执行张量的分片，有点类似于 TP，除了整个张量会及时重建以进行前向或反向计算，因此不需要修改模型。它还支持各种卸载技术以补偿有限的 GPU 内存。\n",
    "Sharded DDP - 是 ZeRO 的各种其他实现所使用的基础 ZeRO 概念的另一个名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6975910-286c-4288-9363-3e402e56d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "推理\n",
    "使用 device_map = 'auto'，to_bettertransformer\n",
    "\n",
    "model = model.to_bettertransformer()\n",
    "\n",
    "model = model.reverse_bettertransformer()\n",
    "model.save_pretrained(\"saved_model\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "\n",
    "\n",
    "max_memory_mapping = {0: \"600MB\", 1: \"1GB\"}\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", load_in_4bit=True, max_memory=max_memory_mapping\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "prompt = \"Hello, my llama is cute\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dace1e-e7de-4d5c-a6a6-617f8cac059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "从 4.18.0 版开始，最终占用超过 10GB 空间的模型检查点会自动分成更小的部分。就拥有一个检查点而言model.save_pretrained(save_dir)，当你这样做时，你最终会得到几个部分检查点（每个检查点的大小 < 10GB）和一个将参数名称映射到它们存储的文件的索引。\n",
    "\n",
    "您可以在使用参数进行分片之前控制最大尺寸max_shard_size，因此为了举例说明，我们将使用分片尺寸较小的正常尺寸模型：让我们采用传统的 BERT 模型。\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    model.save_pretrained(tmp_dir)\n",
    "    print(sorted(os.listdir(tmp_dir)))\n",
    "    \n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
    "    print(sorted(os.listdir(tmp_dir)))\n",
    "    \n",
    "from transformers.modeling_utils import load_sharded_checkpoint\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
    "    load_sharded_checkpoint(model, tmp_dir)\n",
    "    \n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
    "new_model = AutoModel.from_pretrained(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fba2e-8a30-4abc-a788-66cef83bbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            print(logs)\n",
    "            \n",
    " https://github.com/huggingface/transformers/blob/v4.30.0/src/transformers/trainer_callback.py#L227\n",
    "https://github.com/huggingface/transformers/blob/v4.30.0/src/transformers/trainer_callback.py#L227\n",
    "自定义回调函数\n",
    "\n",
    "class MyCallback(TrainerCallback):\n",
    "    \"A callback that prints a message at the beginning of training\"\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"Starting training\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())\n",
    ")\n",
    "\n",
    "trainer = Trainer(...)\n",
    "trainer.add_callback(MyCallback)\n",
    "# Alternatively, we can pass an instance of the callback class\n",
    "trainer.add_callback(MyCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2a9a7-3bb2-4136-b9c4-574b30f467a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitsandbytes 量化\n",
    "# pip install transformers accelerate bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"bigscience/bloom-1b7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
    "\n",
    "#####\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"bigscience/bloom-1b7\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    llm_int8_threshold=10,\n",
    ")\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device_map,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
