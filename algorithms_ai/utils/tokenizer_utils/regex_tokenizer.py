import re


class RegexTokenizer:
    def __init__(self, regex=None, keep_tokens=()):
        # è§„åˆ™åˆ‡è¯ï¼Œå¯ä»¥ä¿ç•™å“ªäº›è¯ä¸ç”¨åˆ‡åˆ†
        # åŒä¸€ä¸ªå­—ç¬¦å¼€å¤´æ»¡è¶³å¤šä¸ªè§„åˆ™ï¼Œä»¥ç¬¬ä¸€ä¸ªè§„åˆ™ä½œæ•°
        if not regex:
            from algorithms_ai.utils.string_utils.common_string import PUNCTUATIONS, HTML_ESCAPE_DICT
            # é»˜è®¤ï¼šæ•°å­—ï¼ˆæ•´æ•°å’Œå°æ•°ï¼‰| è‹±æ–‡å•è¯(è‹±æ–‡è¿å­—ç¬¦åŒ…å«) | ç©ºç™½ç¬¦ | ä¸­æ–‡ | å¸Œè…Šå¤§å†™å­—æ¯ | å¸Œè…Šå°å†™å­—æ¯ | æ ‡ç‚¹ | å…¶ä»–å­—ç¬¦
            regex = f"\d+\.?\d+|[A-Za-z]+|\s+|[\u4e00-\u9fa5]|[\u0391-\u03a9]|[\u03b1-\u03c9]|[{PUNCTUATIONS}]|[^a-zA-Z0-9{PUNCTUATIONS}\s\u4e00-\u9fa5\u03b1-\u03c9\u0391-\u03a9]"
            html_escape = list(HTML_ESCAPE_DICT.keys())
            regex = '|'.join(html_escape) + '|' + regex
        if keep_tokens:
            regex = '|'.join(keep_tokens) + '|' + regex
        self.find_re = re.compile(regex)

    def run(self, text):
        # è¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºåˆ‡åˆ†åçš„æ–‡æœ¬ä»¥åŠåæ ‡--start-end--- æœ€åä¸€ä¸ªå­—ç¬¦çš„åæ ‡ï¼Œè€Œä¸æ˜¯ç´¢å¼•
        texts = []
        offsets_mapping = []
        texts_append = texts.append
        offsets_mapping_append = offsets_mapping.append
        tmp_index = 0

        token_index = 0
        for i in self.find_re.finditer(text):
            if tmp_index < i.start():
                if text[tmp_index:i.start()].strip():
                    texts_append(text[tmp_index:i.start()])
                    offsets_mapping_append((tmp_index, i.start() - 1))
                    token_index += 1
            if i.group().strip():
                texts_append(i.group())
                offsets_mapping_append((i.start(), i.end() - 1))
                token_index += 1
            tmp_index = i.end()

        if (tmp_index < len(text)) and (text[tmp_index:len(text) - 1].strip()):
            texts_append(text[tmp_index:len(text) - 1])
            offsets_mapping_append((tmp_index, len(text) - 1))
            token_index += 1
        return {'texts': texts, 'offsets_mapping': offsets_mapping}


if __name__ == '__main__':
    # s4 = "CX (cisplatin 80 mg/m(2) IV Q3W; capecitabine 1000 mg/m(2) P.O. BID for 14 days Q3W) plus intravenous AMG 386 10 mg/kg QW (Arm A"
    # t = SentenceTokenizer(keep_suffix=('s.v',))
    #
    # print(t.run(s4))
    #
    # s = "Oral acalabruti(nib) 100â€‰mg æˆ‘iåœ¨å‘å•Š <  twic\u00b7e daily  ; was & admi\nnister\ted with or &lt; 13.8kg/m without,Î±-1 12.ğŸ˜Š "
    # w_t = WordTokenizer(regex=None, keep_words=('åœ¨å‘',))
    # print(w_t.run(s))
    # s2 = 'Recent advances in CRISPR/Cas9-mediated knock-ins in mammalian cells.'
    # s2 = '1H-NMR'
    # re2 = f"[A-Za-z0-9]+|\s+|[\u4e00-\u9fa5]|[\u0391-\u03a9]|[\u03b1-\u03c9]"
    # print(WordTokenizer(regex=re2).run(s2))
    #
    # ENGLISH_PUNCTUATION = '!"#$%&\'()*+/:<=>?@[\\]^`{|}~-'
    # CHINESE_PUNCTUATION = 'ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï½Ÿï½ ï½¢ï½£ï½¤\u3000ã€ã€ƒã€ˆã€‰ã€Šã€‹ã€Œã€ã€ã€ã€ã€‘ã€”ã€•ã€–ã€—ã€˜ã€™ã€šã€›ã€œã€ã€ã€Ÿã€°ã€¾ã€¿â€“â€”â€˜â€™â€›â€œâ€â€â€Ÿâ€¦â€§ï¹ï¹‘ï¹”Â·ï¼ï¼Ÿï½¡ã€‚'
    # PUNCTUATIONS = ENGLISH_PUNCTUATION + CHINESE_PUNCTUATION
    #
    # regex = f"[A-Za-z0-9_.;,]+|\s+|[\u4e00-\u9fa5]|[\u0391-\u03a9]|[\u03b1-\u03c9]|[{PUNCTUATIONS}]"
    # find_re = re.compile(regex)
    # print(list(find_re.finditer('Wnt/Î²-cateninh,2.2aç®—æ³•')))
    # print(RegexTokenizer(keep_tokens=('2.2a',)).run('Wnt/Î²-cateninh, 2.2a%'))
    s = '  Terrible swelÎ²li-IIng 12.3up.\nI went   to d&gt;my GPåœ¨å‘å•Š  '
    r = ''
    print(RegexTokenizer(r, keep_tokens=('2.2a',)).run(s))

