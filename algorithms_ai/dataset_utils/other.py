# å¤„ç†å®ä½“è¯†åˆ«çš„label
# è¾“å…¥åŸæ–‡ï¼Œå’Œæ ‡ç­¾æˆ–é¢„æµ‹çš„å®ä½“ï¼Œè¾“å‡ºè§„èŒƒåŒ–åçš„å®ä½“
# åŸå§‹çš„labeléœ€è¦åŒ…å«ï¼šstart_offsetï¼Œend_offsetï¼Œlabel,ä»¥åŠåŸæ–‡


class FormatLabels:
    def __init__(self,
                 left_token_keep=None,
                 right_token_keep=('%',),
                 left_token_remove=None,
                 right_token_remove=None,
                 left_token_add=None,
                 right_token_add=None
                 ):
        """
        é»˜è®¤å¯¹è¾¹ç•Œ('(',')'),('[',']')è¿›è¡Œå¤„ç†ï¼Œ
        æ ¹æ®tokenså’Œlabels-tokenæ¥å¯¹labelè¿›è¡Œè§„èŒƒåŒ–
        left_token_remove: labelä¸­å·¦è¾¹tokençš„å»é™¤ , é»˜è®¤å»é™¤æ‰€æœ‰æ ‡ç‚¹+ç©ºæ ¼
        right_token_remove: labelä¸­å³è¾¹tokençš„å»é™¤ï¼Œ, é»˜è®¤å»é™¤æ‰€æœ‰æ ‡ç‚¹+ç©ºæ ¼

        ä¿ç•™çš„å¼ºåº¦å¤§äºå»é™¤çš„å¼ºåº¦
        left_token_keepï¼šlabelä¸­å·¦è¾¹tokençš„ä¿ç•™,
        right_token_keepï¼šlabelä¸­å³è¾¹tokençš„ä¿ç•™

        # æœ€ååœ¨å·¦å³ä¸¤è¾¹æ·»åŠ token
        left_token_add =None,
        right_token_add = None

        """

        self.left_remove = list(PUNCTUATIONS) + [' ']
        if left_token_remove:
            for i in left_token_remove:
                self.left_remove.append(i)

        self.right_remove = list(PUNCTUATIONS) + [' ']
        if right_token_remove:
            for i in right_token_remove:
                self.right_remove.append(i)

        if right_token_keep:
            for i in right_token_keep:
                if i in self.right_remove:
                    self.right_remove.remove(i)

        if left_token_keep:
            for i in left_token_keep:
                if i in self.left_remove:
                    self.left_remove.remove(i)

        self.left_add = []
        if left_token_add:
            self.left_add.extend(left_token_add)

        self.right_add = []
        if right_token_add:
            self.right_add.extend(right_token_add)

    def run(self, labels, tokens):
        labels = [self.format_each_label(i, tokens) for i in labels]
        labels = [i for i in labels if i]

        all_labels = [i.label for i in labels]
        refined_labels = []
        for a_l in all_labels:
            # ä¸åŒlabelå¤šä¸ªå®ä½“çš„åˆå¹¶
            sub_labels = [i for i in labels if i.label == a_l]
            sub_labels = sorted(sub_labels, key=lambda x: x.start_token.index)
            for i in self.merge_nested_labels(sub_labels):
                if i not in refined_labels:
                    refined_labels.append(i)

        return refined_labels

    def format_each_label(self, label, tokens):
        # ->NER_LABEL
        # å¯¹äºlabelå¢åˆ å·¦å³è¾¹ç•Œ
        while label.start_token.text in self.left_remove:
            label = NER_LABEL(tokens[label.start_token.index + 1], label.end_token, label.label)
            if label.start_token.index > label.end_token.index:  # å…¨éƒ½åˆ å®Œ
                return

        while label.end_token.text in self.right_remove:
            label = NER_LABEL(label.start_token, tokens[label.end_token.index - 1], label.label)
            if label.start_token.index > label.end_token.index:  # å…¨éƒ½åˆ å®Œ
                return

        all_texts = [i.text for i in tokens[label.start_token.index:label.end_token.index + 1]]

        done_bound = False
        while not done_bound:
            c = 0
            # è¡¥å……å·¦å³æ‹¬å·
            for left_bound, right_bound in [('(', ')'), ('[', ']')]:
                if all_texts.count(left_bound) - all_texts.count(right_bound) >= 1:
                    if label.end_token.index <= len(tokens) - 2 and tokens[
                        label.end_token.index + 1].text == right_bound:
                        label = NER_LABEL(label.start_token, tokens[label.end_token.index + 1], label.label)
                        break

                if all_texts.count(right_bound) - all_texts.count(left_bound) >= 1:
                    if label.start_token.index >= 1 and tokens[label.start_token.index - 1].text == left_bound:
                        label = NER_LABEL(tokens[label.start_token.index - 1], label.end_token, label.label)
                        break
                c += 1

            if c == 2:
                done_bound = True

        while (label.start_token.index >= 1) and (tokens[label.start_token.index - 1] in self.left_add):
            label = NER_LABEL(tokens[label.start_token.index - 1], label.end_token, label.label)

        while (label.end_token.index + 1 <= len(tokens)) and (tokens[label.end_token.index + 1] in self.right_add):
            label = NER_LABEL(label.start_token, tokens[label.end_token.index + 1], label.label)

        return label

    @staticmethod
    def merge_nested_labels(labels):
        # NER_LABEL
        # åŒä¸€ç§å®ä½“çš„åµŒå¥—åˆå¹¶ï¼Œè¾“å…¥çš„labelsæœ‰åº ,äº¤å‰çš„åˆå¹¶ï¼Œç›¸é‚»çš„åˆå¹¶
        # TODO: ä¸¤ä¸ªç›¸é‚»ç›¸åŒæ ‡ç­¾çš„å®ä½“ï¼Œå¦‚æœä¸­é—´æ˜¯â€™[,],(,)'å°±æŠŠå®ƒä»¬åˆå¹¶ä¸€èµ·
        refined_labels = []

        last_label = labels[0]
        label = last_label.label
        for current_label in labels[1:]:
            if current_label.start_token.index > last_label.end_token.index + 1:
                refined_labels.append(last_label)
                last_label = current_label
            else:
                last_label = NER_LABEL(
                    last_label.start_token,
                    current_label.end_token,
                    label
                )
        refined_labels.append(last_label)
        return refined_labels

    def get_labels_from_pred_tokens(self, token_predicts):
        # æ¨¡å‹è¾“å‡ºå„ä¸ªtokençš„åˆ¤æ–­ç»“æœï¼Œå°†è¿™äº›ç»“æœåˆå¹¶èµ·æ¥,BIO ï¼Œ Iå’ŒBæœ‰ç›¸ä¼¼åœ°ä½
        all_labels = []
        append_label = all_labels.append
        label = []
        init_label = label.clear
        append_token = label.append

        last_tag = 'O'
        last_label = 'O'
        for token_index, current_pred in enumerate(token_predicts):
            current_tag = current_pred.split('-')[0]
            current_label = current_pred.split('-')[-1]

            if last_tag == 'O':
                if current_tag in ('B', 'I'):
                    append_token(TOKEN(token_index, -1, -1, '', current_label))
            elif last_tag == 'B':
                if current_tag == 'O':
                    append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                elif current_tag == 'B':
                    append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                    append_token(TOKEN(token_index, -1, -1, '', current_label))
                else:  # I
                    if last_label == current_label:
                        append_token(TOKEN(token_index, -1, -1, '', current_label))
                    else:
                        append_label(NER_LABEL(label[0], label[-1], last_label))
                        init_label()
                        append_token(TOKEN(token_index, -1, -1, '', current_label))
            else:  # I
                if current_tag == 'O':
                    append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                elif current_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
                    append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                    append_token(TOKEN(token_index, -1, -1, '', current_label))
                else:  # 'I'
                    if last_label == current_label:
                        append_token(TOKEN(token_index, -1, -1, '', current_label))
                    else:
                        append_label(NER_LABEL(label[0], label[-1], last_label))
                        init_label()
                        append_token(TOKEN(token_index, -1, -1, '', current_label))
            last_tag = current_tag
            last_label = current_label
        if label:
            append_label(NER_LABEL(label[0], label[-1], last_label))
        return all_labels

    def get_labels_from_pred_tokens_strict(self, token_predicts):
        # æ¨¡å‹è¾“å‡ºå„ä¸ªtokençš„åˆ¤æ–­ç»“æœï¼Œå°†è¿™äº›ç»“æœåˆå¹¶èµ·æ¥,BIO ,ä¸¥æ ¼BIO
        all_labels = []
        append_label = all_labels.append
        label = []
        init_label = label.clear
        append_token = label.append

        last_tag = 'O'
        last_label = 'O'
        for token_index, current_pred in enumerate(token_predicts):
            current_tag = current_pred.split('-')[0]
            current_label = current_pred.split('-')[-1]

            if last_tag == 'O':
                if current_tag == 'B':
                    append_token(TOKEN(token_index, -1, -1, '', current_label))
            elif last_tag == 'B':
                if current_tag == 'O':
                    append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                elif current_tag == 'B':
                    append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                    append_token(TOKEN(token_index, -1, -1, '', current_label))
                else:  # I
                    if last_label == current_label:
                        append_token(TOKEN(token_index, -1, -1, '', current_label))
            else:  # I
                if current_tag == 'O':
                    if label:
                        append_label(NER_LABEL(label[0], label[-1], last_label))
                        init_label()
                elif current_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
                    if label:
                        append_label(NER_LABEL(label[0], label[-1], last_label))
                    init_label()
                    append_token(TOKEN(token_index, -1, -1, '', current_label))
                else:  # 'I'
                    if last_label == current_label:
                        if label:
                            append_token(TOKEN(token_index, -1, -1, '', current_label))
            last_tag = current_tag
            last_label = current_label
        if label:
            append_label(NER_LABEL(label[0], label[-1], last_label))

        return all_labels

    def simple_process_input_text(self, input_text):
        # è§„èŒƒåŒ–å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå¾—åˆ°å¤„ç†åçš„æ–‡æœ¬ï¼Œæ–‡æœ¬çš„tokenså’Œå¤„ç†åæ–‡æœ¬çš„tokens
        tokens = self.word_tokenizer(input_text)
        refined_input_text = ''
        last_end = -1

        for token in tokens:
            token_text = self.token_processor(token.text) if self.token_processor(token.text) else ' '
            if token.start_offset == last_end + 1:
                refined_input_text += token_text
            else:
                refined_input_text += (' ' + token_text)
            last_end = token.end_offset

        return refined_input_text.strip()


# class FormatLabels:
#     def __init__(self,
#                  left_token_keep=None,
#                  right_token_keep=('%',),
#                  left_token_remove=None,
#                  right_token_remove=None,
#                  left_token_add=None,
#                  right_token_add=None
#                  ):
#         """
#         é»˜è®¤å¯¹è¾¹ç•Œ('(',')'),('[',']')è¿›è¡Œå¤„ç†ï¼Œ
#         æ ¹æ®tokenså’Œlabels-tokenæ¥å¯¹labelè¿›è¡Œè§„èŒƒåŒ–
#         left_token_remove: labelä¸­å·¦è¾¹tokençš„å»é™¤ , é»˜è®¤å»é™¤æ‰€æœ‰æ ‡ç‚¹+ç©ºæ ¼
#         right_token_remove: labelä¸­å³è¾¹tokençš„å»é™¤ï¼Œ, é»˜è®¤å»é™¤æ‰€æœ‰æ ‡ç‚¹+ç©ºæ ¼
#
#         ä¿ç•™çš„å¼ºåº¦å¤§äºå»é™¤çš„å¼ºåº¦
#         left_token_keepï¼šlabelä¸­å·¦è¾¹tokençš„ä¿ç•™,
#         right_token_keepï¼šlabelä¸­å³è¾¹tokençš„ä¿ç•™
#
#         # æœ€ååœ¨å·¦å³ä¸¤è¾¹æ·»åŠ token
#         left_token_add = None
#         right_token_add = None
#
#         """
#
#         self.left_remove = list(PUNCTUATIONS) + [' ']
#         if left_token_remove:
#             for i in left_token_remove:
#                 self.left_remove.append(i)
#
#         self.right_remove = list(PUNCTUATIONS) + [' ']
#         if right_token_remove:
#             for i in right_token_remove:
#                 self.right_remove.append(i)
#
#         if right_token_keep:
#             for i in right_token_keep:
#                 if i in self.right_remove:
#                     self.right_remove.remove(i)
#
#         if left_token_keep:
#             for i in left_token_keep:
#                 if i in self.left_remove:
#                     self.left_remove.remove(i)
#
#         self.left_add = []
#         if left_token_add:
#             self.left_add.extend(left_token_add)
#
#         self.right_add = []
#         if right_token_add:
#             self.right_add.extend(right_token_add)
#
#     def run(self, labels, tokens):
#         labels = [self.format_each_label(i, tokens) for i in labels]
#         labels = [i for i in labels if i]
#
#         all_labels = [i.label for i in labels]
#         refined_labels = []
#         for a_l in all_labels:
#             # ä¸åŒlabelå¤šä¸ªå®ä½“çš„åˆå¹¶
#             sub_labels = [i for i in labels if i.label == a_l]
#             sub_labels = sorted(sub_labels, key=lambda x: x.start_token.index)
#             for i in self.merge_nested_labels(sub_labels):
#                 if i not in refined_labels:
#                     refined_labels.append(i)
#
#         return refined_labels
#
#     def format_each_label(self, label, tokens):
#         # ->TokenLabel
#         # å¯¹äºlabelå¢åˆ å·¦å³è¾¹ç•Œ
#         while label.start_token.text in self.left_remove:
#             label = TokenLabel(tokens[label.start_token.index + 1], label.end_token, label.label)
#             if label.start_token.index > label.end_token.index:  # å…¨éƒ½åˆ å®Œ
#                 return
#
#         while label.end_token.text in self.right_remove:
#             label = TokenLabel(label.start_token, tokens[label.end_token.index - 1], label.label)
#             if label.start_token.index > label.end_token.index:  # å…¨éƒ½åˆ å®Œ
#                 return
#
#         all_texts = [i.text for i in tokens[label.start_token.index:label.end_token.index + 1]]
#
#         done_bound = False
#         while not done_bound:
#             c = 0
#             # è¡¥å……å·¦å³æ‹¬å·
#             for left_bound, right_bound in [('(', ')'), ('[', ']')]:
#                 if all_texts.count(left_bound) - all_texts.count(right_bound) >= 1:
#                     if label.end_token.index <= len(tokens) - 2 and tokens[
#                         label.end_token.index + 1].text == right_bound:
#                         label = TokenLabel(label.start_token, tokens[label.end_token.index + 1], label.label)
#                         break
#
#                 if all_texts.count(right_bound) - all_texts.count(left_bound) >= 1:
#                     if label.start_token.index >= 1 and tokens[label.start_token.index - 1].text == left_bound:
#                         label = TokenLabel(tokens[label.start_token.index - 1], label.end_token, label.label)
#                         break
#                 c += 1
#
#             if c == 2:
#                 done_bound = True
#
#         while (label.start_token.index >= 1) and (tokens[label.start_token.index - 1] in self.left_add):
#             label = TokenLabel(tokens[label.start_token.index - 1], label.end_token, label.label)
#
#         while (label.end_token.index + 1 <= len(tokens)) and (tokens[label.end_token.index + 1] in self.right_add):
#             label = TokenLabel(label.start_token, tokens[label.end_token.index + 1], label.label)
#
#         return label
#
#     @staticmethod
#     def merge_nested_labels(labels):
#         # TokenLabel
#         # åŒä¸€ç§å®ä½“çš„åµŒå¥—åˆå¹¶ï¼Œè¾“å…¥çš„labelsæœ‰åº ,äº¤å‰çš„åˆå¹¶ï¼Œç›¸é‚»çš„åˆå¹¶
#         # TODO: ä¸¤ä¸ªç›¸é‚»ç›¸åŒæ ‡ç­¾çš„å®ä½“ï¼Œå¦‚æœä¸­é—´æ˜¯â€™[,],(,)'å°±æŠŠå®ƒä»¬åˆå¹¶ä¸€èµ·
#         refined_labels = []
#
#         last_label = labels[0]
#         label = last_label.label
#         for current_label in labels[1:]:
#             if current_label.start_token.index > last_label.end_token.index + 1:
#                 refined_labels.append(last_label)
#                 last_label = current_label
#             else:
#                 last_label = TokenLabel(
#                     last_label.start_token,
#                     current_label.end_token,
#                     label
#                 )
#         refined_labels.append(last_label)
#         return refined_labels
#
#     def get_labels_from_pred_tokens(self, token_predicts):
#         # æ¨¡å‹è¾“å‡ºå„ä¸ªtokençš„åˆ¤æ–­ç»“æœï¼Œå°†è¿™äº›ç»“æœåˆå¹¶èµ·æ¥,BIO ï¼Œ Iå’ŒBæœ‰ç›¸ä¼¼åœ°ä½
#         all_labels = []
#         append_label = all_labels.append
#         label = []
#         init_label = label.clear
#         append_token = label.append
#
#         last_tag = 'O'
#         last_label = 'O'
#         for token_index, current_pred in enumerate(token_predicts):
#             current_tag = current_pred.split('-')[0]
#             current_label = current_pred.split('-')[-1]
#
#             if last_tag == 'O':
#                 if current_tag in ('B', 'I'):
#                     append_token(Token(token_index, -1, -1, '', current_label))
#             elif last_tag == 'B':
#                 if current_tag == 'O':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                 elif current_tag == 'B':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # I
#                     if last_label == current_label:
#                         append_token(Token(token_index, -1, -1, '', current_label))
#                     else:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                         init_label()
#                         append_token(Token(token_index, -1, -1, '', current_label))
#             else:  # I
#                 if current_tag == 'O':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                 elif current_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # 'I'
#                     if last_label == current_label:
#                         append_token(Token(token_index, -1, -1, '', current_label))
#                     else:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                         init_label()
#                         append_token(Token(token_index, -1, -1, '', current_label))
#             last_tag = current_tag
#             last_label = current_label
#         if label:
#             append_label(TokenLabel(label[0], label[-1], last_label))
#         return all_labels
#
#     def get_labels_from_pred_tokens_strict(self, token_predicts):
#         # æ¨¡å‹è¾“å‡ºå„ä¸ªtokençš„åˆ¤æ–­ç»“æœï¼Œå°†è¿™äº›ç»“æœåˆå¹¶èµ·æ¥,BIO ,ä¸¥æ ¼BIO
#         all_labels = []
#         append_label = all_labels.append
#         label = []
#         init_label = label.clear
#         append_token = label.append
#
#         last_tag = 'O'
#         last_label = 'O'
#         for token_index, current_pred in enumerate(token_predicts):
#             current_tag = current_pred.split('-')[0]
#             current_label = current_pred.split('-')[-1]
#
#             if last_tag == 'O':
#                 if current_tag == 'B':
#                     append_token(Token(token_index, -1, -1, '', current_label))
#             elif last_tag == 'B':
#                 if current_tag == 'O':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                 elif current_tag == 'B':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # I
#                     if last_label == current_label:
#                         append_token(Token(token_index, -1, -1, '', current_label))
#             else:  # I
#                 if current_tag == 'O':
#                     if label:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                         init_label()
#                 elif current_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
#                     if label:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # 'I'
#                     if last_label == current_label:
#                         if label:
#                             append_token(Token(token_index, -1, -1, '', current_label))
#             last_tag = current_tag
#             last_label = current_label
#         if label:
#             append_label(TokenLabel(label[0], label[-1], last_label))
#
#         return all_labels


# supplement label

# class LabelProcessor:
#     def __init__(self,
#                  left_token_keep=None,
#                  right_token_keep=('%',),
#                  left_token_remove=None,
#                  right_token_remove=None,
#                  left_token_add=None,
#                  right_token_add=None
#                  ):
#         """
#         é»˜è®¤å¯¹è¾¹ç•Œ('(',')'),('[',']')è¿›è¡Œå¤„ç†ï¼Œ
#         æ ¹æ®tokenså’Œlabels-tokenæ¥å¯¹labelè¿›è¡Œè§„èŒƒåŒ–
#         left_token_remove: labelä¸­å·¦è¾¹tokençš„å»é™¤ , é»˜è®¤å»é™¤æ‰€æœ‰æ ‡ç‚¹+ç©ºæ ¼
#         right_token_remove: labelä¸­å³è¾¹tokençš„å»é™¤ï¼Œ, é»˜è®¤å»é™¤æ‰€æœ‰æ ‡ç‚¹+ç©ºæ ¼
#
#         ä¿ç•™çš„å¼ºåº¦å¤§äºå»é™¤çš„å¼ºåº¦
#         left_token_keepï¼šlabelä¸­å·¦è¾¹tokençš„ä¿ç•™,
#         right_token_keepï¼šlabelä¸­å³è¾¹tokençš„ä¿ç•™
#
#         # æœ€ååœ¨å·¦å³ä¸¤è¾¹æ·»åŠ token
#         left_token_add =None,
#         right_token_add = None
#
#         """
#
#         self.left_remove = list(PUNCTUATIONS) + [' ']
#         if left_token_remove:
#             for i in left_token_remove:
#                 self.left_remove.append(i)
#
#         self.right_remove = list(PUNCTUATIONS) + [' ']
#         if right_token_remove:
#             for i in right_token_remove:
#                 self.right_remove.append(i)
#
#         if right_token_keep:
#             for i in right_token_keep:
#                 if i in self.right_remove:
#                     self.right_remove.remove(i)
#
#         if left_token_keep:
#             for i in left_token_keep:
#                 if i in self.left_remove:
#                     self.left_remove.remove(i)
#
#         self.left_add = []
#         if left_token_add:
#             self.left_add.extend(left_token_add)
#
#         self.right_add = []
#         if right_token_add:
#             self.right_add.extend(right_token_add)
#
#     def run(self, labels, tokens):
#         labels = [self.format_each_label(i, tokens) for i in labels]
#         labels = [i for i in labels if i]
#
#         all_labels = [i.label for i in labels]
#         refined_labels = []
#         for a_l in all_labels:
#             # ä¸åŒlabelå¤šä¸ªå®ä½“çš„åˆå¹¶
#             sub_labels = [i for i in labels if i.label == a_l]
#             sub_labels = sorted(sub_labels, key=lambda x: x.start_token.index)
#             for i in self.merge_nested_labels(sub_labels):
#                 if i not in refined_labels:
#                     refined_labels.append(i)
#
#         return refined_labels
#
#     def format_each_label(self, label, tokens):
#         # ->TokenLabel
#         # å¯¹äºlabelå¢åˆ å·¦å³è¾¹ç•Œ
#         while label.start_token.text in self.left_remove:
#             label = TokenLabel(tokens[label.start_token.index + 1], label.end_token, label.label)
#             if label.start_token.index > label.end_token.index:  # å…¨éƒ½åˆ å®Œ
#                 return
#
#         while label.end_token.text in self.right_remove:
#             label = TokenLabel(label.start_token, tokens[label.end_token.index - 1], label.label)
#             if label.start_token.index > label.end_token.index: # å…¨éƒ½åˆ å®Œ
#                 return
#
#         all_texts = [i.text for i in tokens[label.start_token.index:label.end_token.index + 1]]
#
#         done_bound = False
#         while not done_bound:
#             c = 0
#             # è¡¥å……å·¦å³æ‹¬å·
#             for left_bound, right_bound in [('(', ')'), ('[', ']')]:
#                 if all_texts.count(left_bound) - all_texts.count(right_bound) >= 1:
#                     if label.end_token.index  <= len(tokens) - 2 and tokens[label.end_token.index + 1].text == right_bound:
#                         label = TokenLabel(label.start_token, tokens[label.end_token.index + 1], label.label)
#                         break
#
#                 if all_texts.count(right_bound) - all_texts.count(left_bound) >= 1:
#                     if label.start_token.index >= 1 and tokens[label.start_token.index - 1].text == left_bound:
#                         label = TokenLabel(tokens[label.start_token.index - 1], label.end_token, label.label)
#                         break
#                 c += 1
#
#             if c == 2:
#                 done_bound = True
#
#         while (label.start_token.index >= 1) and (tokens[label.start_token.index - 1] in self.left_add):
#             label = TokenLabel(tokens[label.start_token.index - 1], label.end_token, label.label)
#
#         while (label.end_token.index + 1 <= len(tokens)) and (tokens[label.end_token.index + 1] in self.right_add):
#             label = TokenLabel(label.start_token, tokens[label.end_token.index + 1], label.label)
#
#         return label
#
#     @staticmethod
#     def merge_nested_labels(labels):
#         # TokenLabel
#         # åŒä¸€ç§å®ä½“çš„åµŒå¥—åˆå¹¶ï¼Œè¾“å…¥çš„labelsæœ‰åº ,äº¤å‰çš„åˆå¹¶ï¼Œç›¸é‚»çš„åˆå¹¶
#         # TODO: ä¸¤ä¸ªç›¸é‚»ç›¸åŒæ ‡ç­¾çš„å®ä½“ï¼Œå¦‚æœä¸­é—´æ˜¯â€™[,],(,)'å°±æŠŠå®ƒä»¬åˆå¹¶ä¸€èµ·
#         refined_labels = []
#
#         last_label = labels[0]
#         label = last_label.label
#         for current_label in labels[1:]:
#             if current_label.start_token.index > last_label.end_token.index+1:
#                 refined_labels.append(last_label)
#                 last_label = current_label
#             else:
#                 last_label = TokenLabel(
#                     last_label.start_token,
#                     current_label.end_token,
#                     label
#                 )
#         refined_labels.append(last_label)
#         return refined_labels
#
#     def get_labels_from_pred_tokens(self, token_predicts):
#         # æ¨¡å‹è¾“å‡ºå„ä¸ªtokençš„åˆ¤æ–­ç»“æœï¼Œå°†è¿™äº›ç»“æœåˆå¹¶èµ·æ¥,BIO ï¼Œ Iå’ŒBæœ‰ç›¸ä¼¼åœ°ä½
#         all_labels = []
#         append_label = all_labels.append
#         label = []
#         init_label = label.clear
#         append_token = label.append
#
#         last_tag = 'O'
#         last_label = 'O'
#         for token_index, current_pred in enumerate(token_predicts):
#             current_tag = current_pred.split('-')[0]
#             current_label = current_pred.split('-')[-1]
#
#             if last_tag == 'O':
#                 if current_tag in ('B', 'I'):
#                     append_token(Token(token_index, -1, -1, '', current_label))
#             elif last_tag == 'B':
#                 if current_tag == 'O':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                 elif current_tag == 'B':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # I
#                     if last_label == current_label:
#                         append_token(Token(token_index, -1, -1, '', current_label))
#                     else:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                         init_label()
#                         append_token(Token(token_index, -1, -1, '', current_label))
#             else:  # I
#                 if current_tag == 'O':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                 elif current_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # 'I'
#                     if last_label == current_label:
#                         append_token(Token(token_index, -1, -1, '', current_label))
#                     else:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                         init_label()
#                         append_token(Token(token_index, -1, -1, '', current_label))
#             last_tag = current_tag
#             last_label = current_label
#         if label:
#             append_label(TokenLabel(label[0], label[-1], last_label))
#         return all_labels
#
#     def get_labels_from_pred_tokens_strict(self, token_predicts):
#         # æ¨¡å‹è¾“å‡ºå„ä¸ªtokençš„åˆ¤æ–­ç»“æœï¼Œå°†è¿™äº›ç»“æœåˆå¹¶èµ·æ¥,BIO ,ä¸¥æ ¼BIO
#         all_labels = []
#         append_label = all_labels.append
#         label = []
#         init_label = label.clear
#         append_token = label.append
#
#         last_tag = 'O'
#         last_label = 'O'
#         for token_index, current_pred in enumerate(token_predicts):
#             current_tag = current_pred.split('-')[0]
#             current_label = current_pred.split('-')[-1]
#
#             if last_tag == 'O':
#                 if current_tag == 'B':
#                     append_token(Token(token_index, -1, -1, '', current_label))
#             elif last_tag == 'B':
#                 if current_tag == 'O':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                 elif current_tag == 'B':
#                     append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # I
#                     if last_label == current_label:
#                         append_token(Token(token_index, -1, -1, '', current_label))
#             else:  # I
#                 if current_tag == 'O':
#                     if label:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                         init_label()
#                 elif current_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
#                     if label:
#                         append_label(TokenLabel(label[0], label[-1], last_label))
#                     init_label()
#                     append_token(Token(token_index, -1, -1, '', current_label))
#                 else:  # 'I'
#                     if last_label == current_label:
#                         if label:
#                             append_token(Token(token_index, -1, -1, '', current_label))
#             last_tag = current_tag
#             last_label = current_label
#         if label:
#             append_label(TokenLabel(label[0], label[-1], last_label))
#
#         return all_labels


"""
{
  "æ–‡æœ¬å¤„ç†": [
    "tokenåˆ‡åˆ†å™¨,WordTokenizer-tokenize_text",
    "tokenå¤„ç†å™¨,TokenProcessor - process_token_text",
    "æ–‡æœ¬å¤„ç†æŠŠåŸå§‹æ–‡æœ¬å¤„ç†æˆtokençº§åˆ«çš„æ–‡æœ¬ï¼Œä»¥åŠtokençš„æ˜ å°„ï¼Œç„¶åå¯¹åŸå§‹çš„æ ‡ç­¾è¿›è¡Œtokençš„æ˜ å°„ï¼Œå¾—åˆ°labelçš„tokenè¡¨ç¤ºï¼Œrefined_label"
  ],
  "ä¸­é—´": "å¯¹æ–‡æœ¬å¤„ç†å¾—åˆ°æ–‡æœ¬tokenæ˜ å°„å’Œrefined-token,è‹¥æœ‰labelï¼Œå¯ä»¥æŠŠlabelè§„èŒƒåŒ–ä¸ºlist(token)æ ¼å¼",
  "å¤šç§å®ä½“åˆ†å®ä½“è¿›è¡Œå¤„ç†":{
    "è¾“å…¥è¾“å‡º": "è¾“å…¥çš„æ˜¯input_textå’Œlabelï¼Œä¸­é—´refined_text è¾“å‡ºrefined_label",
    "æ ‡ç­¾å¤„ç†": [
    "æ ‡ç­¾çš„ä¿®æ­£ï¼Œä¸»è¦è¾¹ç•Œçš„å¤„ç†ï¼Œå·¦å³è¾¹ç•Œçš„å¢åˆ æ”¹æŸ¥ï¼Œå’Œä¸­é—´tokençš„åˆ‡åˆ†",
    "æ ‡ç­¾çš„åˆ é™¤ï¼Œä¸»è¦åˆ é™¤é”™è¯¯çš„æ ‡ç­¾--åˆ¤æ–­--è¾“å…¥labelå’Œrefined_text,å¾—å‡ºtrue/false",
    "æ ‡ç­¾çš„è¡¥å……ï¼Œä¸»è¦ä½¿ç”¨è§„åˆ™æŠ½å–å®ä½“--ï¼Œä¸€èˆ¬ä½¿ç”¨reè§„åˆ™ï¼Œä½¿ç”¨refined_input_textï¼Œé€šè¿‡æ­£åˆ™åŒ¹é…å¾—å‡ºçš„ç»“æœè¿˜è¦æ˜ å°„æˆtokenå½¢å¼",
    "æ ‡ç­¾çš„èåˆï¼Œä¸€èˆ¬ç›¸é‚»æˆ–äº¤å‰å˜æˆä¸€ä¸ª,--è¾“å…¥å¤šä¸ªå®ä½“"
  ],
"å¤šå®ä½“å¤„ç†": "èåˆå¤šå®ä½“ï¼Œæˆ–ä¿ç•™",
"desc": "é€šè¿‡å¤„ç†refined_input_textå’Œlabelï¼Œå¾—åˆ° refined_label"},
  "æœ€å": "å¯ä»¥é€šè¿‡input_text,å’Œrefined_labelå¾—åˆ°åŸå§‹labelï¼›",
  "å¦å¤–": "å¦‚æœåŠ å…¥æ¨¡å‹ï¼Œè¾“å…¥åŸå§‹æ–‡æœ¬ï¼Œå¤„ç†æˆrefined_input_textï¼Œè¿›å…¥æ¨¡å‹ï¼Œè¾“å‡ºrefined_label,å†è¿›å…¥æ ‡ç­¾å¤„ç†ï¼Œæœ€åå°†å¤„ç†åçš„æ ‡ç­¾è¾“å‡º",
  "æ‰€ä»¥": "æœ‰å››ä¸ªå˜é‡:input_text,refined_input_text,label,refined_label,è®­ç»ƒæˆ–è¿›å…¥æ¨¡å‹ç”¨refined_input_textå’Œrefined_labelï¼Œè¾“å‡ºç»™ç”¨æˆ·æ˜¯input_textå’Œlabel",
  "å…¶ä»–": "ä¸­é—´æ¶‰åŠåˆ°çš„å˜é‡ï¼ŒåŸå§‹å’Œç»“æœæ–‡æœ¬tokençš„æ˜ å°„ï¼štokensï¼Œmapping_tokens",
  "æ³¨æ„": "è¦æ·»åŠ logå’Œdebugä¿¡æ¯"
}


"""


"""
class TmpToken:
    def __init__(self, pred='O', token_text='[init_token]', tmp_start=0, text='xxxx'):
        self.entity_type_tag = pred[0]
        self.entity_type = pred[2:]
        self.start_offset, self.end_offset = self._get_local(token_text, text, tmp_start)  # åæ ‡

    def _get_local(self, token_text, raw_text, tmp_start):
        tmp_start = tmp_start + 1
        sub_text = raw_text[tmp_start:]
        if token_text.startswith('##') and len(token_text) > 2:  # tokenæ˜¯åˆ‡åˆ†çš„
            token_text = token_text[2:]
        token_text_index = sub_text.find(token_text.lower())

        if token_text_index == -1:  # tokenä¸åœ¨åŸæ–‡ä¸­
            return 'not_found', 'not_found'
        else:  # æ­£å¸¸token
            return token_text_index + tmp_start, token_text_index + tmp_start + len(token_text) - 1

    def __str__(self):
        return str({
            'start_offset': self.start_offset,
            'end_offset': self.end_offset,
            'entity_type': self.entity_type,
            'entity_type_tag': self.entity_type_tag
        })


def refined_entity(entity_tokens: list, text: str):
    # è§„èŒƒåŒ–tokenå®ä½“
    return {"start_offset": (start_offset := max(0, entity_tokens[0].start_offset)),
            "end_offset": (end_offset := min(entity_tokens[-1].end_offset, len(text) - 1)),  # åæ ‡
            "text": text[start_offset:end_offset + 1],
            "label": entity_tokens[0].entity_type}


# å°†é¢„æµ‹çš„logistå€¼æ˜ å°„ä¸ºlabelæ ¼å¼,å¤„ç†çš„æ–‡æœ¬æ˜¯é¢„å¤„ç†åçš„è¾“å…¥æ¨¡å‹çš„æ–‡æœ¬ï¼Œä¸æ˜¯åŸå§‹æ–‡æœ¬
def align_labels_by_token(preds_list, tokens_list, preprocessed_text):
    text = preprocessed_text.lower()
    tmp_end_offset = -1
    last_token = TmpToken('O', '[CLS]', tmp_end_offset, text)
    entities = []  # å­˜å‚¨æ‰€æœ‰å®ä½“çš„å®ä½“token
    append_entity = entities.append
    entity = []  # å­˜å‚¨å•ä¸ªå®ä½“çš„æ‰€æœ‰token
    init_entity = entity.clear
    append_token = entity.append

    # try:
    for token, pred in zip(tokens_list[1:-1], preds_list[1:-1]):
        current_token = TmpToken(pred, token, tmp_end_offset, text)
        if current_token.start_offset == 'not_found':  # è·³è¿‡é”™è¯¯çš„token
            continue
        if last_token.entity_type_tag == 'O':
            if current_token.entity_type_tag == 'B':
                append_token(current_token)
            elif current_token.entity_type_tag == 'I':
                append_token(current_token)
        elif last_token.entity_type_tag == 'B':
            if current_token.entity_type_tag == 'O':
                append_entity(refined_entity(entity, preprocessed_text))
                init_entity()
            elif current_token.entity_type_tag == 'B':
                if last_token.entity_type == current_token.entity_type:
                    append_token(current_token)
                else:
                    append_entity(refined_entity(entity, preprocessed_text))
                    init_entity()
                    append_token(current_token)
            else:  # 'I'
                if current_token.entity_type == last_token.entity_type:
                    append_token(current_token)
                else:
                    append_entity(refined_entity(entity, preprocessed_text))
                    init_entity()
                    append_token(current_token)
        else:  # 'I'
            if current_token.entity_type_tag == 'O':
                append_entity(refined_entity(entity, preprocessed_text))
                init_entity()
            elif current_token.entity_type_tag == 'B':  # å¦ä¸€ä¸ªå®ä½“
                if current_token.entity_type == last_token.entity_type:
                    append_token(current_token)
                else:
                    append_entity(refined_entity(entity, preprocessed_text))
                    init_entity()
                    append_token(current_token)
            else:  # 'I'
                if current_token.entity_type == last_token.entity_type:
                    append_token(current_token)
                else:
                    append_entity(refined_entity(entity, preprocessed_text))
                    init_entity()
                    append_token(current_token)
        last_token = current_token
        tmp_end_offset = current_token.end_offset
    if entity:
        append_entity(refined_entity(entity, preprocessed_text))
    return entities

"""

if __name__ == '__main__':
    # s = 'With median follow-up of 21 months, 24-month relapse-free survival (RFS) was 67% (95% CI 62% to 73%) in the 326 patients.'
    # NerFormat().run(s,[])
    s2 = "Oral acalabruti(nib) 100â€‰mg æˆ‘iåœ¨å‘å•Š <  twic\u00b7e daily  ; was & admi\nnister\ted with or &lt; 13.8kg/m without,Î±-1 12.ğŸ˜Š "
    labels = [{'start_offset': 14, 'end_offset': 18, 'text': 'i(nib', 'label': 'a'},  # end_offseté”™è¯¯
              # {'start_offset': 16, 'end_offset': 20, 'text': 'nib)', 'label': 'a'},  # æ­£ç¡®
              {'start_offset': 38, 'end_offset': 44, 'text': 'wice dail', 'label': 'c'},  # è¾¹ç•Œä¸å¯¹
              {'start_offset': 46, 'end_offset': 54, 'text': 'ly  ; ', 'label': 'c'},
              {'start_offset': 79, 'end_offset': 89, 'text': '&lt; 13.8', 'label': 'd'}]
    s = [{'start_offset': 5, 'end_offset': 18, 'text': 'acalabruti(nib', 'label': 'a'},
         {'start_offset': 38, 'end_offset': 54, 'text': 'twice daily ; was', 'label': 'c'},
         {'start_offset': 78, 'end_offset': 87, 'text': 'or < 13.8', 'label': 'd'}]

    # s2 = "sd fatigue (62 [55.4%]) and gynecomastia (41 [36.6%])sdf "
    # s = {'id': '18612151',
    #      'arms_ner': [{'start_offset': 173,
    #                    'end_offset': 216,
    #                    'text': '(paclitaxel plus carboplatin) with cetuximab',
    #                    'label': 'arm_option'}],
    #      'input_text': 'Two hundred twenty-nine chemotherapy-naive patients with advanced-stage NSCLC were enrolled onto a phase II selection trial evaluating sequential or concurrent chemotherapy (paclitaxel plus carboplatin) with cetuximab.'}
    # labels = [{"start_offset": 3, "end_offset": 125-77+3, "text": "fatigue (62 [55.4%]) and gynecomastia (41 [36.6%]", "label": "result"}]
    print(NER_Processor().process_raw_data(s2, labels))
    print(NER_Processor().text_processor.simple_process_input_text(s2))
    print(s2[78:87])
    print(NER_Processor().get_raw_labels(s2, s))
    k = 'Oral acalabruti(nib) 100 mg æˆ‘iåœ¨å‘å•Š < twicÂ·e daily ; was & admi nister ed with or < 13.8kg/m without,Î±-1 12.ğŸ˜Š'
    print(k[78:87])
